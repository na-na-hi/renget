!!!note running Locals via Colab / `Enter the Maretrix` edition

[TOC4]

#### readme
> what it is?
it a guide on running Locals via [Google Colab](https://colab.research.google.com/) + all the settings + knobs + presets + two pony-based LoRAs by [anon from /ppp/ thread](https://desuarchive.org/mlp/thread/40255207/#40271923)
***

> context size?
for most models - **7000 tokens in total** (scaled). set the following:
* `Response Length (tokens)` = 300-500
* `Context Size (tokens)` = 7000

for **EXL2** models - **4000 tokens in total**. set the following:
* `Response Length (tokens)` = 300-500
* `Context Size (tokens)` = 4000
***

> what is Google Colab? do I need a good PC for this?
Colab is *Google's data-servers*. Google allows people to use them for free in non-commercial purpose (accent on *research and learning*). paid accounts may use Colab in various other purposes and have more performance/priority. you need internet connection only: **your PC/GPU/CPU/RAM don't matter**. all computation is done solely on Colab servers
***

> what is the catch?
==**time limit**==. Google Colab applies a vague time limit allowing only a certain number of hours per account. **usually** you can use Google Colab **for 2-3 hours per day**, but it varies greatly due to unknown factors. one time it allowed me to sit for 7 hours and in other time cut me short after 55 minutes
***

> but they count only active usage time, right? if I launch their system and will not use the model then I don't use their resources, correct?
**no**. Google counts idle connection as a real connection regardless of active usage. in simple terms: if you have connected to Google Colab and left your PC for four hours and literally did nothing - you will get 'daily limit is reached' nonetheless
***

> what will happen after my time limit is up?
Google will cut off your access to their T4 GPU. and Locals do require GPU for work. you can still use Colab on CPU but models will not load. your **usage limit will recharge in 8-12 hours**. other than rate limit there is no other limitation and anon who made a well-known rentry about Colab [claim to use it for Models for months](https://desuarchive.org/g/thread/95472866/#95480712) with no problems
![](https://cdn.discordapp.com/attachments/1152584624367734907/1152594819491835924/gpu_limit.png)
***

> can I abuse it with multi-accounting? will Google Colab ban me for that?
yes, you can do multi-accounting and abuse their free limits, but it is highly recommended to use VPN + fake user-agent / incognito mode to protect yourself
***

> NSFW?
**yes**. no strings attached. Locals are absolutely uncensored so you can do whatever you want
***

> will Google ban me for NSFW?
Google will not ban your account for NSFW: but they can in theory throttle or rate limit accounts for suspicious usage; however I yet to find the real evidence of such practice. still don't push very questionable content too much
***

> ...but Google banned Pygmalion...
\[[more info here](#about-pygmalion-rant)\]
tl;dr: it was done in the different (much stupider) time, and it no longer applies to current time. nowadays users can freely run Pygmalion 13B on Colab without negative consequences 
***

> ...but Google banned A1111...
not banned but severely handicapped with throttling; because everyone uses it! [Derpibooru alone has 8000+ AI pics](https://www.derpibooru.org/search?q=ai+content%2C+-generator%3Anovelai) and you can guess with high certainty that 80% of them were generated via Colab, and that number doesn't include 800.000+ bad pics that were discarded in process. now apply those numbers to the whole internet, not 4chan only, and you get a picture. too much payload for Google (and people were abusing it much more than Pygmalion)
***
***
***

#### notebook
##### launch
==**--\> #1**==. download [that notebook](https://cdn.discordapp.com/attachments/1152584624367734907/1153008238204293263/chg44_nb_exl2.ipynb)
just to clarify: the code is taken from [that pony-related notebook](https://colab.research.google.com/drive/1Un02h4uQN6zLlgL3zmMOeyce9ICEA3qO) and [notebook from colabfreellamas](https://rentry.org/colabfreellamas) 
 
 
==**--\> #2**==. proceed to [Google Colab](https://colab.research.google.com/) (requires gmail account)
 
 
==**--\> #3**==. `File` -\> `Open notebook` -\> `Upload` -\> ==Select notebook==

![](https://cdn.discordapp.com/attachments/1152584624367734907/1152643088905416725/howto_notebook_open.png)

if you cannot find `File` then click on the *gray arrow* in top-right corner:

![](https://cdn.discordapp.com/attachments/1152584624367734907/1152643847365607424/image.png)
 
 
==**--\> #4**==. you will be redirected on that page.  check the settings on the right:

![](https://cdn.discordapp.com/attachments/1152584624367734907/1152644309649203240/image.png)

|setting                       |description|
|----|----|
|`text_streaming`|self-explanatory. set it to `true` if you need streaming, but requests get ~20% longer in my tests|
|`GetAPI`|set it to `true`. use `false` only if you want to use some other web-interface for Colab|
|`huggingface_org`|\[[more details](#models)\] those three options allow you to select what model you want to load. only **GPTQ** and **low-bit EXL2** are supported** by that notebook. ==set to Mythomax by default== |
|`huggingface_repo`|^|
|`huggingface_branch`|^|
|`use_LoRA`|\[[more details](#loras)\] set it to `true` if you want MLP-related LoRAs for your model. LoRA is text data which acts as addon/plugin for the basic model providing more text and content. ==set to FIMFarchive-v1 by default== |
|`LoRA`|two LoRAs are available: 1) "**desu-/mlp/-v1**" - archive of /mlp/ board with various greentexts and anons' faggotry, and 2) "**FIMFarchive-v1**" - archive of various ponyfics from FIMfiction|

if you cannot see the settings then click there:

![](https://cdn.discordapp.com/attachments/1152584624367734907/1152660192832540772/image.png)
 
 
==**--\> #5**==. in *top-right corner* look for `reconnect` link, next to it *an arrow-down* -\> `Change runtime type` -\> `Hardware accelerator` -> `T4 GPU`

![](https://cdn.discordapp.com/attachments/1152584624367734907/1152662754478534757/howto_notebook_gpu.png)

> I get "RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver" error
you didn't select `T4 GPU` in `Hardware accelerator`, check pic above. **or** your *daily limit is run out* and you are now trying to connect via CPU, in that case wait for 8-12 hours before trying again
 
 
==**--\> #6**==. press on the **first PLAY button** to enable music player. it is a 24h silent soundfile to keep the tab busy and prevent its hibernation (and issues with Colab)

![](https://cdn.discordapp.com/attachments/1152584624367734907/1152663490637602906/howto_notebook_music.png)
 
 
==**--\> #7**==. press on the **second PLAY button** and wait. it will take ~90 seconds to start the server

![](https://cdn.discordapp.com/attachments/1152584624367734907/1152668781034881104/howto_notebook_run.png)
 
 
==**--\> #8**==. at the end you will get two links that look like this:
* `https://****************.trycloudflare.com/api` 
	-- **main link**
* `wss://****************.trycloudflare.com/api/v1/stream` 
	-- **streaming link** (if you need streaming, optional)

![](https://cdn.discordapp.com/attachments/1152584624367734907/1152668952401563878/howto_notebook_link.png)
 
 
==**--\> #9**==. in **SillyTavern** select `Text Gen WebUI (ooba/Mancer)` as API and then paste both links:
* **main link** goes into `Blocking API url`
* **streaming link** goes into `Streaming API url` (if you need streaming, optional)
--after that click `connect`, ensure that the `name of Model` appeared below... **and chat with AI Mares** (or >no hooves, not judging)

![](https://cdn.discordapp.com/attachments/1152584624367734907/1152669820995784754/howto_notebook_link.png)
 
in **Agnai** create the following preset:
* select `AI Service` -\> `Kobold / 3rd party`
* `Kobold / 3rd-party Format` -\> `Textgen (Ooba)`
* `Third Party URL` -\> your link from Colab **WITHOUT `API` AT THE END**
* `Disable Auto-URL`: OFF (gray)

one more time:
instead of 
`https://****************.trycloudflare.com/api`
paste
`https://****************.trycloudflare.com/`

![](https://cdn.discordapp.com/attachments/1152584624367734907/1152799196404011078/agnai_1.png)

> I am getting "Failed to generate response: Textgen request failed (404): Not Found error"
you copied the link with API at then, remove it
 
 
==**--\> #10**==. **don't close the tab, left it open**
 
 
==**--\> #11**==. setup [Advanced Formatting](#advanced-formatting) 
 
 
==**--\> #12**==. setup [Knobs](#knobs)
 
 
==**--\> #13**==. when you done chatting. in *top-right corner* look for `reconnect` link, next to it *an arrow-down* -\> `Disconnect and delete runtime`

![](https://cdn.discordapp.com/attachments/1152584624367734907/1152670471083536404/image.png)

***
***
***

##### models
that notebook supports:
* any 13B models **of GPTQ format** - it automatically scales the **context size to 7000 token**
* low-bit 13B models  **of EXL2 format** - it automatically scales the **context size to 4000 token**

browse [huggingface/models](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending&search=GPTQ+13B) for various 13B GPTQ models. when you found an interesting model paste its data into `huggingface_org`, `huggingface_repo` and `huggingface_branch`. follow the pic below:

![](https://cdn.discordapp.com/attachments/1152584624367734907/1152672412358430720/howto_model_params.png)

***
if you absolutely don't know what model to pick then check [/lmg/ rentry with the leaderboard](https://rentry.org/ayumi_erp_rating#13b-models) (but don't take it as absolute recommendation). some good options to start with:
 
==**--\> [Stheno L2 13B by Sao10K](https://huggingface.co/Sao10K/Stheno-L2-13B)**==
**Stheno**, produces good writing and steers into the dialogues, shows good reasoning for 13B model
* `huggingface_org` - TheBloke
* `huggingface_repo` - Stheno-L2-13B-GPTQ
* `huggingface_branch` - main

==**--\> [MLewdBoros L2 13B by Undi95](https://huggingface.co/Undi95/MLewdBoros-L2-13B)**==
**MLewdBoros** is the most depraved and NSFW-centered 13B Local which is trained specifically on smut and sexo --but surprisingly is doing good narrative too
* `huggingface_org` - TheBloke
* `huggingface_repo` - MLewdBoros-L2-13B-GPTQ
* `huggingface_branch` - main

==**--\> [MythoMax L2 13B by Gryphe](https://huggingface.co/Gryphe/MythoMax-L2-13b)**==
**MythoMax** is a little 13B full of big sovl. maybe not as smart or as lewd, but produces surprisingly poetic and rich descriptions and can surprise the reader
* `huggingface_org` - TheBloke
* `huggingface_repo` - MythoMax-L2-13B-GPTQ 
* `huggingface_branch` - main

==**--\> [MythoMax L2 13B by Gryphe/QMB15](https://huggingface.co/QMB15/mythomax-L2-13B-4.625bit-exl2)**==
**EXL2**-version of **MythoMax**, which suppose to be much smarter. LoRAs will not work under that model
* `huggingface_org` - QMB15
* `huggingface_repo` - mythomax-L2-13B-4.625bit-exl2
* `huggingface_branch` - main


***
***
***
##### LoRAs
that notebook supports two pony-related LoRAs that can be appplied to any GPTQ model. LoRAs are created [by anon in /ppp/ thread](https://desuarchive.org/mlp/thread/40255207/#40271923)
* ==**FIMFarchive-v1**== - dataset of fanfics from FIMfiction archive
* ==**desu-/mlp/-v1**== - dataset of /mlp/ posts, including greentext

!!!warning at this moment of writing **LoRAs cannot be applied to EXL2** models

LoRA is an subset of data which contains examples, extra text, weight, lore, training corpus and extra details. they can be served as the plugins that can be directly incorporated into any model, or serve as extra knowledge cutoff for models. in that notebook we are using the last approach and apply them on a fly

those two LoRAs *improve models' awareness about MLP and help with lingo and names*. **reasoning itself is unaffected** so don't expect models to become much smarter, but they will be able, for example, to deduce that Element of Honesty is Applejack (with certain probability). on pic below you can see the **effect of applied FIMFarchive-v1** LoRA (before/after):

![](https://cdn.discordapp.com/attachments/1152584624367734907/1152760670211821628/compare_loras.png)
-> *(Scootaloo is RD's adoptive daughter is too cute, I don't care)* ->

the negative consequence of LoRAs is spurting of random data from dataset (noise tokens), which can be **mitigated with lower temperature** or more aggressive sampling. **just cut them off**. in my test they mostly proc on the low context prompt until the model get enough tokens to works with

![](https://cdn.discordapp.com/attachments/1152584624367734907/1152680765709107292/howto_lora_noise.png)

...however some of those OOC are nonironically hilarious and I don't even consider them as the bad thing:

![](https://cdn.discordapp.com/attachments/1152584624367734907/1152681096014729276/howto_lora_kek.png)
***
below are pictures for comparisons, done on basic **Athena**, then with two different LoRAs applied, and forth column for **MythoMax** for extras:

|Athena v1 Model|Athena v1 Model + desu-/mlp/-v1|Athena v1 Model + FIMFarchive-v1|MythoMaxL2 Model + FIMFarchive-v1|
|---|---|---|---|
|![](https://cdn.discordapp.com/attachments/1152584624367734907/1152681789383512247/ex1_norm.png)|![](https://cdn.discordapp.com/attachments/1152584624367734907/1152681789651943495/ex1_chag.png)|![](https://cdn.discordapp.com/attachments/1152584624367734907/1152681789916188793/ex1_fim.png)|![](https://cdn.discordapp.com/attachments/1152584624367734907/1152681790167859360/ex1_Myth.png)|
|![](https://cdn.discordapp.com/attachments/1152584624367734907/1152683017576402974/ex2_norm.png)|![](https://cdn.discordapp.com/attachments/1152584624367734907/1152683018025181194/ex2_chag.png)|![](https://cdn.discordapp.com/attachments/1152584624367734907/1152683018255880315/ex2_fim.png)|![](https://cdn.discordapp.com/attachments/1152584624367734907/1152683018507526144/ex2_Myth.png)|
|![](https://cdn.discordapp.com/attachments/1152584624367734907/1152683342773374976/ex3_norm.png)|![](https://cdn.discordapp.com/attachments/1152584624367734907/1152683342991458345/ex3_chag.png)|![](https://cdn.discordapp.com/attachments/1152584624367734907/1152683343209570344/ex3_fim.png)|![](https://cdn.discordapp.com/attachments/1152584624367734907/1152683343549321276/ex3_Myth.png)|
|![](https://cdn.discordapp.com/attachments/1152584624367734907/1152683550521446460/ex4_norm.png)|![](https://cdn.discordapp.com/attachments/1152584624367734907/1152683550831804507/ex4_chag.png)|![](https://cdn.discordapp.com/attachments/1152584624367734907/1152683551133814855/ex4_fim.png)|![](https://cdn.discordapp.com/attachments/1152584624367734907/1152683551762944071/ex4_Myth.png)|
|![](https://cdn.discordapp.com/attachments/1152584624367734907/1152683759615881378/ex5_norm.png)|![](https://cdn.discordapp.com/attachments/1152584624367734907/1152683758701527070/ex5_chag.png)|![](https://cdn.discordapp.com/attachments/1152584624367734907/1152683758995124224/ex5_fim.png)|![](https://cdn.discordapp.com/attachments/1152584624367734907/1152683759238398174/ex5_Myth.png)|
|![](https://cdn.discordapp.com/attachments/1152584624367734907/1152683935768252586/ex6_norm.png)|![](https://cdn.discordapp.com/attachments/1152584624367734907/1152683936321896488/ex6_chag.png)|![](https://cdn.discordapp.com/attachments/1152584624367734907/1152683936711987262/ex6_fim.png)|![](https://cdn.discordapp.com/attachments/1152584624367734907/1152683937064292372/ex6_Myth.png)|

***
***
***
#### Advanced Formatting
##### tldr
!!!note don't want to read all that stuff and just need a copy-paste of settings?
    * [freellamas](https://rentry.org/freellamas)
    * [tsukasa13b](https://rentry.org/tsukasa13b)
    * [my preset below](#my-settings)
    * @todo: need more

to properly use Local models you need to set correct options for them. open `Advanced Formatting` tab in SillyTavern and set the following options:

***
##### Story String
it acts as your `MAIN` and templates formatting (*Agnai user find it very familiar*)
the absolute minimalist, super default, `Story String` looks like this. it basically goes: if there is `system` info then output it, if there is `description` info then output it, etc...
```
{{#if system}}{{system}}{{/if}}
{{#if wiBefore}}{{wiBefore}}{{/if}}
{{#if description}}{{description}}{{/if}}
{{#if personality}}{{personality}}{{/if}}
{{#if scenario}}{{scenario}}{{/if}}
{{#if wiAfter}}{{wiAfter}}{{/if}}
{{#if persona}}{{persona}}{{/if}}
```
|option|description|
|----|----|
|`system`|content of `system prompt` below \[[read more](#system-prompt)\]
|`wiBefore`|content of `World Info` / `LoreBook` if you set to insert it BEFORE `character's card`|
|`wiAfter`|content of `World Info` / `LoreBook` if you set to insert it AFTER `character's card`|
|`description`|content of `character's card` (defs)|
|`personality`|content of character's `Personality summary` (from Advanced Definition)|
|`scenario`|content of character's `Scenario ` (from Advanced Definition)|
|`persona`|content of user's `Persona Description` (from Persona Management)|

you can move those templates as you want and maybe find the variant that works the best for you and your model. for example:
```
{{#if scenario}}{{scenario}}{{/if}}
{{#if wiBefore}}{{wiBefore}}{{/if}}
{{#if description}}{{description}}{{/if}}
{{#if personality}}{{personality}}{{/if}}
{{#if wiAfter}}{{wiAfter}}{{/if}}
{{#if persona}}{{persona}}{{/if}}
{{#if system}}{{system}}{{/if}}
```
...furthermore you can add extra data to (hopefully) aid AI:
```
{{#if system}}### Roleplay instructions:
{{system}}{{/if}}

{{#if scenario}}### Roleplay Scenario:
{{/scenario}}{{/if}}

{{#if description}}### {{char}}'s description:
{{description}}{{/if}}

{{#if persona}}### {{user}}'s description:
{{persona}}{{/if}}

{{#if personality}}### {{char}}'s personality:
{{personality}}{{/if}}

### Roleplay facts and memory:
{{#if wiBefore}}{{wiBefore}}{{/if}}
{{#if wiAfter}}{{wiAfter}}{{/if}}
```
...or even wrap them into XML-tags:
```
<character_description>
{{#if description}}{{description}}{{/if}}
</character_description>

<user_description>
{{#if persona}}{{persona}}{{/if}}
</user_description>

<character_personality>
{{#if personality}}{{personality}}{{/if}}
</character_personality>

<roleplay_scenario>
{{#if scenario}}{{scenario}}{{/if}}
</roleplay_scenario>

<roleplay_facts>
{{#if wiAfter}}{{wiAfter}}{{/if}}
{{#if wiBefore}}{{wiBefore}}{{/if}}
</roleplay_facts>

<roleplay_instruction>
{{#if system}}{{system}}{{/if}}
</roleplay_instruction>
```
...in addition you can manually add your instructions here and force `Story String` into `MAIN`; also some people recommend add `### Input:` at the start which supposedly helps AI --but can be a snake oil. for example:
```
### Input:

This is a back and forth roleplay between Human and you based on My Little Pony (MLP). You are an author, tasked with crafting a captivating, memorable narrative based on the provided instructions and chat. Human writes for {{user}}; while you write for {{char}}, other characters, and narrate roleplay. Throughout the roleplay, describe every action in vivid details, and use dialogue effectively to advance the plot. Remember, the goal is to leave a lasting impression on Human.

{{#if description}}{{description}}{{/if}}
{{#if personality}}{{personality}}{{/if}}
{{#if persona}}{{persona}}{{/if}}
{{#if scenario}}{{scenario}}{{/if}}
{{#if wiBefore}}{{wiBefore}}{{/if}}
{{#if wiAfter}}{{wiAfter}}{{/if}}

Follow there guidelines:
{{#if system}}{{system}}{{/if}}
```

for **Agnai** use the following settings:
* `Use Prompt Template`: ON (red)
* `Prompt Template` ([source](https://rentry.org/tsukasa13b)):

```
<|system|>{{#if system_prompt}}{{system_prompt}}{{/if}}

{{char}}'s Persona: {{personality}}

{{#if example_dialogue}}
{{char}} talks like: {{example_dialogue}}
{{/if}}

{{#if scenario}}
This scenario of the conversation is: {{scenario}}
{{/if}}

{{#each msg}}{{#if .isbot}}<|model|>{{/if}}{{#if .isuser}}<|user|>{{/if}}{{.name}}: {{.msg}}
{{/each}}
{{#if ujb}}<|system|>{{ujb}}{{/if}}
<|model|>{{post}}
```

***
##### System Prompt
`System Prompt` will be inserted in place of `{{system}}` template in `Story String`. you may utilize it as `MAIN` that will go first before anything else:

* `Story String` (start):
```
{{#if system}}{{system}}{{/if}}

{{#if wiBefore}}{{wiBefore}}{{/if}}
<...>
```
* `System Prompt`:
``` js
This is a back and forth roleplay between Human and AI based on My Little Pony (MLP).
```
...or use at the end of `Story String` and utilize as a pseudo `AN` with extra instructions:

* `Story String` (end):
```
<...>
{{#if wiAfter}}{{wiAfter}}{{/if}}

{{#if system}}{{system}}{{/if}}
``` 
* `System Prompt`:
``` js
Back-n-forth roleplay / Stay in character / Provide details imagery full of action and movements / Include dialogues and environment / Use knowledge of MLP and equine anatomy.
```
--whatever approach to use depends on your personal preference, current RP and the current model

***
##### Example Separator and Chat Start
* `Example Separator` is added before every provided `Dialogue Examples`
* `Chat Start` is added before the actual `Chat` started (after all the examples)

...those two commands aim to help AI into difference what it the part of actual narrative and what serves as an example
![](https://cdn.discordapp.com/attachments/1152584624367734907/1152699120448983120/image.png)

a typical guideline is to use the following separators:
``` js
Example Separator: ### Example Dialogue:
       Chat Start: ### New Roleplay:
```
I personally like the following approach - to announce that all examples are done and now we proceed to the roleplay itself:
``` js
Example Separator - ### Example Dialogue Start:
       Chat Start - ### Example Dialogue End. New Roleplay Begins:
```

***
##### Instruct mode and Sequences
!!!warning **ENABLE `INSTRUCT MODE` AND DON'T DISABLE IT**. 
	we need that option for AI to generate text instead of continuing it

* `Input Sequence` is added before every User's response (in chat history)
* `Output Sequence` is added before every AI's response (in chat history)

...those two commands aim to help AI into difference the roles in the story
![](https://cdn.discordapp.com/attachments/1152584624367734907/1152702618322346014/image.png)

a typical guideline is to use the following sequences:
``` js
 Input Sequence: ### Instruction:
Output Sequence: ### Response:
```

...but you can use whatever format you want and maybe model will click better for you, some examples:
``` js
 Input Sequence: Human:
Output Sequence: AI:

 Input Sequence: ---Human turn:
Output Sequence: ---AI turn:

 Input Sequence: ### {{user}}:
Output Sequence: ### {{char}}:
```
***
* `First Output Sequence` is added before very *first* AI's response
* `Last Output Sequence` is added before very *last* AI's response

...usually you don't need to touch `First Output Sequence` but for `Last Output Sequence` you can add an extra instructions just before prompt generation, for example:
``` js
Last Output Sequence: ### Response (write as a talented MLP fanfic author):
Last Output Sequence: ### Response (your answer shall be brief and concise):
Last Output Sequence: ### Response (reply in Old English with a mix of UPPERCASE words to empathize Luna's aloofness and her royal Canterlot voice):
```
...of course you can go completely overboard and do stuff like this, and consider it to be a `mini-JB`:
``` js
Last Output Sequence: ### Response (Now it is your time to reply. Follow the following rules and instructions:

- Maintain spatial understanding to ensure extremely realistic scenes and anatomically correct character interactions. 
- Pay attention to each characters species. Never use human anatomy for MLP characters. 
- MLP characters don't wear clothes. 
- Pony characters use ponyfied language. 
- Obey MLP canon. Your TOP PRIORITY is to make sure that EACH character stays as true to their canon appearances, personalities, mannerisms, and speech patterns as possible. 
- Use English language creatively, in vivid details.

Proceed with your answer):
```

***
##### Tokenizer and padding
!!!warning Set `Tokenizer` to `Sentencepiece (LLaMA)`
proper Tokenizer is required for the correct token for `Response Length (tokens)` and `Context Size (tokens)` correctly. `Sentencepiece` works with all Locals
***
`padding` is a *"token reserve"* to avoid overflow. with *lower values* AI will try to output as many tokens without considering an actual length of prompt, with *higher values* AI will lookahead and finish its prompt prematurely before it overflow the completion

usually setting `padding` to `64` or `96` is enough

***
##### Extra commands
**Wrap Sequences with Newline**
\- that option adds the newlines before sequences. without it they will append on the same lines. good for content clarity

![](https://cdn.discordapp.com/attachments/1152584624367734907/1152708054794649662/asdwrwfw.png)

***
**Replace Macro in Sequences**
\- that option forces SillyTavern to ALWAYS replace {{char}} and {{user}} tags to their respective correct names. if you disable it then instead of character's name that actual words {{char}} and {{user}} will be send

***
**Include Names**
\- that option will append character's name (for both {{user}} and {{char}}) in prompt. it is great thing for RP since it helps AI to follow the roles better, but if you are doing a story-writing (via Writer) then you *probably better to turn them off*

![](https://cdn.discordapp.com/attachments/1152584624367734907/1152709295763357757/dsdsds.png)


***
##### my settings
those are my settings I am using for **Mythomax**. not necessary saying those are the best settings and might be an overkill --but that is what I have settled with. use pic below for reference. I am using newlines to readability and they ain't necessary:

* Story String:
```
{{#if system}}{{system}}{{/if}}
### INPUT:
{{#if description}}### {{char}}'s description: {{description}}{{/if}}
{{#if personality}}### {{char}}'s personality: {{char}}'s personality: {{personality}}{{/if}}
{{#if persona}}### {{user}}'s description: {{persona}}{{/if}}
{{#if scenario}}### Story Scenario: {{scenario}}{{/if}}
{{#if wiBefore}}### Story facts and memory: {{wiBefore}}{{/if}}
{{#if wiAfter}}### Story facts and memory: {{wiAfter}}{{/if}}
```
* System Prompt: 
``` js
Below is an instruction that describes a task. Write a response that appropriately completes the request.
You're {{char}} in this open-ended story that leaves a lasting impression on {{user}}.
Never skip or gloss over any actions. Progress the scene at a naturally slow pace.
```
* misc: 
``` js
   Example Separator: - Example dialogue below. Use it as style and writing reference:
          Chat Start: - Example dialogues end there. Don't mention information mentioned above in the  actual story. ### STORY BEGINS THERE:
      Input Sequence: User:
     Output Sequence: AI:
Last Output Sequence: Play the role of {{char}}. Do not write dialogues and narration for {{user}}. Pay attention to each characters species. ever use human anatomy for MLP characters. Obey MLP canon, appearances, personalities, mannerisms, and speech patterns. Continue the story further while applying your knowledge about MLP and equines. AI:
```
![](https://cdn.discordapp.com/attachments/1152584624367734907/1152823145628184596/my_advanced_settings.png)

#### Knobs
##### tldr
!!!note don't want to read all that stuff and just need a copy-paste of settings?
    * [freellamas](https://rentry.org/freellamas)
    * [tsukasa13b](https://rentry.org/tsukasa13b)
    * [my preset below](#my-preset)
    * @todo: need more

Big Models support mostly `Temperature`, `Top_P`, `Top_K` -and in case of GPT4 - `Frequency penalties`
Locals, in other hand, support much more knobs and offer a huge assortment of options which can make your head dizzy, but things ain't hard
!!!warning **ENABLE `DO SAMPLING` IN OPTIONS**, otherwise most of those settings will not work you

##### presets
SillyTavern offers prebuilt presets you may load and use without figuring what all those knobs do. 

![](https://cdn.discordapp.com/attachments/1152584624367734907/1152725072956633128/todo_presets.png)

one [anon made a good rounddown of most presets](https://rentry.org/llm-settings#presets-presets-everywhere). my imho - for **Athena** and **Mythomax** the best presets are:
- Titanic 
- Space Alien
- Naive
- TFS-with-Top-A
- Midnight Enigma

***
##### Temperature (temp)
your absolute basic knob. it **determinate the pool of tokens (words)**, and their overall number:
- `higher`: more tokens in the pool, more random tokens
- `lower`: less tokens in the pool, more predictable tokens
- `disable`: 1 (default)

at value `temp: 0` AI becomes 100% *deterministic* and always outputs the same text regardless of the seed or swipe

there are two main approaches:
- *set temp at high value* (**1.4+**) and then use other knobs *generously* to trim unnecessary tokens
- *set temp at low value* (**0.6+**) and then use other knobs *sparingly* to avoid trimming good tokens

overall, a safe `temp` value is in the **0.8-1.2** range, depending on how much creativity you need. if you are going with high `temp` then you need to counter-balance it with other sampling methods below

***
##### Top P & Top K
`temp`'s two younger sisters that always follow her around

`Top P` (aka "nucleus sampling") sets the range of tokens to be applied into the pool. it **sums the probability of tokens** until it reaches the point when the total sum of picked tokens exceeds the given range - then **discards all leftover tokens**. so it leaves only the tokens (the mass) of highly probable tokens
- `higher`: more random tokens (less sampling effect)
- `lower`: more predictable tokens (more sampling effect)
- `disable`: 1

`Top K` is dumbed-down `Top P`. instead of actually calculating the sum of tokens' probability - it just **picks the given number of tokens** in a row and **cuts everything else**. in nutshell it limits the total range of pool to that many tokens
- `number`: allow that many tokens into selection
- `disable`: 0

if you set `temp: 2` (max number of random tokens) and set `Top K: 1` (only 1 most likely token to pick from) --then AI becomes fully deterministic because you effectively cut the whole pool to 1 most-likely token which always will be the same

neither of `Top P` and `Top K` is ideal because `Top P` tends to put random tokens into pool, while `Top K` cuts quite likely tokens. pic below explains the shortsights of both samplings. `Top A` below combines both their methods and 'patches' their issue

however, it is still **quite effective idea to set `Top K` to some high value (50-80)** to cut unnecessary tokens firsthoof, and allow other samplings to work with more relevant data

![](https://cdn.discordapp.com/attachments/1152584624367734907/1152730637938204814/topp_k.png)


***
##### Top A
`Top A` is in nutshell a combination of both `Top P` & `Top K`. it uses the mass of the most likely token (the most probable) and verifies other tokens based on it. so instead of using some arbitrary sums - **it uses the chance of the most likely token to cut the pool**. due to its design it is very aggressive and can lead to dry results even on high settings
- `higher`: more predictable tokens (more sampling effect)
- `lower`: more random tokens (less sampling effect)
- `disable`: 0

![](https://cdn.discordapp.com/attachments/1152584624367734907/1152735260606464032/top-a.png)

***
##### Typical P
`Typical P` (aka 'locally typical sampling') is unorthodox knob. it **uses an entropy value to cut both tokens with too low probability and too high probability**. it uses an idea that natural human speech uses median words, where every token has about the same probability as the previous one (aka, *uses tropes and cliches*). the value of that sampling method determines how strong the entropy to look for. while it is good *very good for summarization* and providing facts - **the usage in RP or storywriting can cause more harm than good**
- `higher`: more random tokens (less sampling effect)
- `lower`: more predictable tokens (more sampling effect)
- `disable`: 1

***
##### Tail Free Sampling
`Tail Free Sampling` (TFS) is extremely complicated sampling method **based on the differences between the tokens' probability**. in the first round `TFS` calculates the difference between tokens' probability and *normalizes* them to median (calculates the median of all differences). in the second round it cuts off the tokens behind the threshold. the common idea is **to not set it too low** unless user needs deterministic text
- `higher`: more random tokens (less sampling effect)
- `lower`: more predictable tokens (more sampling effect)
- `disable`: 1

***
##### Epsilon Cutoff & Eta Cutoff
are is basically the **same thing as `Tail Free Sampling` but separated in two different values**. see how in `TFS` the two rounds were mentioned? `Epsilon Cutoff` determines how aggressive the normalization across the board will be, while `Eta Cutoff` sets the threshold of tokens to cut
- `higher`: more predictable tokens (more sampling effect)
- `lower`: more random tokens (less sampling effect)
- `disable`: 0
 
***
##### Repetition Penalty
`Repetition Penalty` **reduces the appearance of tokens that are already present in the text**. if user already has the word '*ministration*' in the prompt (20 sentences ago) then this knob will check whether another appearance of '*ministration*' is allowed. while it sounds as a nice feature, too high value will lead to unnatural text full of synonyms and simile
- `higher`: less word repetition allowed
- `lower`: more word repetition allowed
- `disable`: 1

***
##### Repetition Penalty Range 
`Repetition Penalty Range` tells **how far back AI shall look for the repetitive words** (in tokens)
- `number`: take that many tokens into consideration when determinate repetitions
- `disable`: 0 (all prompt)

***
##### No Repeat Ngram Size 
`Ngram` is sorta plugin for repetition detection and **sets the number of tokens in a row to check for the repetition**. for example the phrase '*the night is still young*' is 6 tokens long. if you set this knob to '6' then AI will prevent the generation of that phrase in text (based on `Repetition Penalty`'s aggressiveness), but setting it to '5' will allow that phrase to generate further. while useful, again, too lower value *will make the text incomprehensible*
- `number`: check for that many tokens in a row for repetition
- `disable`: 0 

***
##### Encoder Rep. Pen.
`Encoder Repetition Penalty` is a special knob that tries to fight with an issue of text becoming incomprehensible with other repetition penalties. it **takes into account the context and allows some tokens that would have been otherwise banned** to slip it. in other words, that option prevents tokens from being banned under regular circumstances if it will improve text legibility
- `higher`: more word repetition
- `lower`: less word repetition (set to 0.8 and have a laugh)
- `disable`: 1

***
##### Negative Prompt & CFG Scale
!!!warning Negative prompts & CFG don't work in free Google Colab because of insufficient VRAM
`Negative prompt` **tells AI what TO NOT generate**. you can do lots of things there:
* give a *general direction* of what you don't want to see (per @ada1's preset):
```
anthro, anthropomorphic, Equestria Girls, tropes, bland, poor characterization, summary, logical error, illogical, incoherent, unintelligible, inarticulate, incomprehensible, out of character, omnipresent, omniscient, summary, forum post, article, OOC, give pony characters human anatomy
```
* tell AI what it *shall not* to do:
```
Speak for {{user}} and describe its actions.
```
* provide the *text with bad grammar* and bad sentence structure ([source](https://ask-max.neocities.org/prompt-buffet/)):
```
he walked. he ran. he screamed. he was lost. it was hopeles. he said, “theres no way out." she said, “ur doomed lol”
```

`CFG Scale` affects **how powerful the negative prompt** must be:
- `higher`: stay away from negative prompt as much as possible
- `lower`: embrace and apply negative prompt into generation
- `disable`: 1

"*why in the name of Celestia's second horn anyone want to apply negative prompt*", you may ask. the answer is simple - you can provide a (literally) random text into that field and set AI to apply it, then it will act as a second-hand randomizer/seed, ([source](https://rentry.org/primeanon#primer_1)) - which is a damn good hack

***
##### Beam search
!!!warning Beams don't work in free Google Colab because of insufficient VRAM
`Beams` allow AI to **predict multiple sentence generation** and then pick the one generation that offers the best token probabilities. it results in generating more organic text, but uses a huge amount of VRAM. ironically but AI can generate much worse text in return because it will look for more organic text rather than the more creative and avoid unsure tokens

`Number of Beams` tells **how many different variants to generate** in total, while `Length Penalty` determines **the length of each generation**

***
##### Penalty Alpha
`Penalty Alpha` flipflops the whole sampling system and **adds a natural degeneration into the range allowing some level of mistake into the range**. now all sampling methods will allow a **margin of error** to (supposedly) make generation more natural, creative, random and unpredictable
- `higher`: bigger margin of error, less predictable text
- `lower`: smaller margin of error, more predictable text
- `disable`: 0

***
##### Mirostat 
infamous `Mirostat`. in nutshell, very-simple, it is **an auto-pilot mode that controls the sampling and adapts the generation based on the currently picked token buffed by randomness factor**. some people enjoy it because it is a simple plug-n-play knob that does most of the work by itself, while other dislike it because of randomness in generation

the basic idea behind of Mirostat goes like this. your usual token prediction and selection work as usual but Mirostat checks what token was picked: 
* if AI picked the token with **high probability** then **Mirostat slightly boosts the chance of all other tokens** in the next selection
* if AI picked the token with **low probability** then **Mirostat slightly lowers the chance of other tokens, but leave the most likely intact** in next selection, allowing AI to pick the token with higher chance next time

Mirostat always adapts the token chance trying to balance between selecting random and predictable tokens. you can say that Mirostat works like the swing, picking between the tokens of both good and bad, and plays a devil's advocate in between

`Mirostat Eta` (learning rate) sets **the overall agressiveness of Mirostat**, how much it will affect the tokens and how often it will affect the generation
- `higher`: more random tokens
- `lower`: more predictable tokens
- `disable`: 0

`Mirostat Tau` sets the amplitide that Mirostat will uses on tokens with each generation (**changing their chance**)
- `higher`: more random tokens
- `lower`: more predictable tokens
- `disable`: 0

`Mirostat Mode` sets what version of Mirostat to use:
- `0`: disable
- `1`: mode for llama.cpp
- `2`: mode for exllama_hf (**Colab notebook works on that**)

because of how `Mirostat` works user need to note a few discretions:
* setting `Top K` to **any value** will help to prevent picking absolutely wrong tokens
* very high `Temp`, without any other sampling, will lead to exceptionally schizo text with powerful `Mirostat` (making Chester look sane)
* `Mirostat` pretty much shows a middle finger to `Repetition Penalty` so expect repetive tokens

setting the correct value for `Mirostat` is not easy and depends on the model and other sampling methods. some good variations from the anons in /lmg/:
``` js
Tau   Eta
  3   0.1 
  3   0.3
  4   0.2
  5   0.1
  6   0.4
```

***
##### my preset
those are my settings I am using for **Mythomax** and **Stheno**. not necessary saying those are the best settings --but that is what I have settled with:

for regular narrative and storywriting. works the best at low context. `Mirostat Tau` is set at somewhat high value but low `Mirostat Eta` compensates it. they both exist here to add some randomness into writing and maybe steer the story into unexpected direction. `Repetition Penalty` is set at low since we are on low context and some repetition are alright, and a bit of `Encoder Repetition Penalty` helps with overall coherence
```
Temperature: 0.77
Repetition Penalty: 1.06
Repetition Penalty Range: 1200.00
Encoder Repetition Penalty: 1.02
No Repeat Ngram Size: 0.00
Top K: 30.00
Top P: 0.90
Typical P: 1.00
Top A: 0.00
Tail Free Sampling: 0.90
Epsilon Cutoff: 0.00
Eta Cutoff: 0.00
Number of Beams: 1.00
Length Penalty: 1.00
Penalty Alpha: 0.00
Mirostat Mode: 2.00
Mirostat Tau: 4.00
Mirostat Eta: 0.15
```

alternative 'default' preset that works the best for Stheno. higher `Temp` with more aggressive `Top P` and `Mirostat` can help into producing more creative results while not diminishing the intelligence
```
Temperature: 0.88
Repetition Penalty: 1.05
Repetition Penalty Range: 1200.00
Encoder Repetition Penalty: 1.01
No Repeat Ngram Size: 0.00
Top K: 35.00
Top P: 0.73
Typical P: 1.00
Top A: 0.00
Tail Free Sampling: 0.92
Epsilon Cutoff: 0.00
Eta Cutoff: 0.00
Number of Beams: 1.00
Length Penalty: 1.00
Penalty Alpha: 0.00
Mirostat Mode: 2.00
Mirostat Tau: 3.50
Mirostat Eta: 0.20
```

preset that works better on text with enough context (2000+ tokens). higher `Temperature` helps to avoid loops as well as higher `Mirostat Eta`. `Repetition Penalty` is adjusted to fight repetitiveness as well
```
Temperature: 0.85
Repetition Penalty: 1.08
Repetition Penalty Range: 1200.00
Encoder Repetition Penalty: 1.02
No Repeat Ngram Size: 0.00
Top K: 30.00
Top P: 0.90
Typical P: 1.00
Top A: 0.00
Tail Free Sampling: 0.90
Epsilon Cutoff: 0.00
Eta Cutoff: 0.00
Number of Beams: 1.00
Length Penalty: 1.00
Penalty Alpha: 0.00
Mirostat Mode: 2.00
Mirostat Tau: 3.50
Mirostat Eta: 0.20
```

when I need extra schizo or spark of divine creativity I use this. it turns text into gibberish on the small prompts (when there is not enough tokens), but works good on the big prompts of 4000+ tokens). `Temperature` cranked very high with idea that `Typical P` will cut noise tokens while leaving some juice ideas intact. `Mirostat` boosted as well just for the heck of it. it is more a creative/random prompt than coherent but can work wit the scenario cards
```
Temperature: 1.40
Repetition Penalty: 1.03
Repetition Penalty Range: 1200.00
Encoder Repetition Penalty: 1.03
No Repeat Ngram Size: 0.00
Top K: 40.00
Top P: 1.00
Typical P: 0.65
Top A: 0.00
Tail Free Sampling: 0.92
Epsilon Cutoff: 0.00
Eta Cutoff: 0.00
Number of Beams: 1.00
Length Penalty: 1.00
Penalty Alpha: 0.00
Mirostat Mode: 2.00
Mirostat Tau: 5.00
Mirostat Eta: 0.30
```

***
***
***
#### misc
##### about Pygmalion (rant)
\[[return back](#readme)\]
take in account the environment back then. end April - early May: GPT4 wasn't around, Claude was available via Slack only and Spermack/Slaude didn't get fruition yet, Turbo-scam was just killed by OpenAI, Locals were awful, Scale-scam was killed by Wang, Roco/Todd died, Chub was just-just created, and people still tried to rizz CAI. in that exceptionally doom-ey time, of course, *majority of users were using Pygmalion on Colab*, and it was the meta and people abused it (and lamented how bad it is), the whole W++ things started because of Pygmalion back then)

Pygmalion was the first case when Colab, openly and undisguised, was used for NSFW. it was the first such case for Google and they over-reacted because they didn't want such notorious fame for their service and media exposure. Google didn't want any of this and banned Pygmalion from Colab

...only Google didn't actually ban Pygmalion. they banned one string - `PygmalionAI` - if that string has appeared in notebook then script wouldn' work. but user can clone Pygmalion repo, rename their model into `TotallyNotLewd` and continue using it on Colab without issues. authors of Pygmalion didn't want to go that way and respected Google's decision - getting a bad reputation of 'Don Quixote tilting at Google' wasn't in their interests either

in the next five months since the incident *the situation was changed completely*: AI became a new normal and Google ain't bother that much already, plus their T4 GPU (on free accounts) are cheap and they care not about its inappropriate usage anymore, needlessly to say that CEO of CAI became Times' TOP100 in 2023 which only solidified the necessity of chat bots for various purpose

Pygmalion ban was a matter of old times because it still was a wild-wild frontier and Google didn't know how to handle the situation better and Pygmalion's team didn't want to push forward or evade ban (which they absolutely can do)

#### credits, backlinks & shoutout
* [colabfreellamas](https://rentry.org/colabfreellamas)
* authors of two Colab notebooks: [one](https://files.catbox.moe/jgqjre.ipynb) & [two](https://files.catbox.moe/kd79da.ipynb)
* author of [two pony LoRAs](https://desuarchive.org/mlp/thread/40255207/#40271923) and [pony-related Colab notebook](https://colab.research.google.com/drive/1Un02h4uQN6zLlgL3zmMOeyce9ICEA3qO)
* author of two pony-trained LLaMA 2 13B models: [LL2-13B-DesuMLP-QLORA-GGML](https://huggingface.co/Nikolai1902/LL2-13B-DesuMLP-QLORA-GGML) & [LL2-13B-FIMFiction-QLORA-GGML](https://huggingface.co/Nikolai1902/LL2-13B-FIMFiction-QLORA-GGML)
* author of [pony-related corpus data](https://huggingface.co/tekkithorse)
* [ada1's preset](https://desuarchive.org/mlp/thread/40293067/#40311295)

* [freellamas](https://rentry.org/freellamas)
* [llm-settings](https://rentry.org/llm-settings)
* [hostfreellamas](https://rentry.org/hostfreellamas)
* [primeanon](https://rentry.org/primeanon)
* [darkstardestinations](https://darkstardestinations.com/parametersTab)
* [ask-max](https://ask-max.neocities.org/presets/#mythomax)

-> **contact::** -> 
-> ==raremew@proton.me== ->
-> Discord: ==rarestMeow#0836== ->