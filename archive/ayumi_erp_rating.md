# Ayumi's LLM Role Play & ERP Ranking (Version 3)

This ranking table contains a rating of different LLMs, which tries to determine which model is most suitable for (erotic) role playing (ERP) by using an automated benchmark. Unfortunately this automated benchmarks has it's limits, but the table can serve as a starting point for you to look for LLM models to try out.

***
[TOC]
***

!!! danger Interpretation Warning: *Writing quality is not covered!*
    **Disclaimer:** This benchmark makes no statement about how well a LLM will be able to drive the story forward. It can also not determine coherency within a longer role play chat. The generated **text quality is not tested for**. For more information look in these sections: [Known Flaws of the ALC-IQ](https://rentry.co/ayumi_erp_rating#known-flaws-of-the-alc-iq) and [Known Flaws of the ERP Score](https://rentry.co/ayumi_erp_rating#known-flaws-of-the-erp-score-and-erp-variety-score)

**##################**

The **most up to date table and changelog** you can find on my new landing page: **http://ayumi.m8geil.de/**

**##################**

| Column | Description |
|----|----|
| ALC-IQ3 | The ALC-IQ3 is the 3rd version of the ALC-IQ. It tries to determine how well a model understands a character card. The higher the better. Best score is 100. |
| ERP3 Score | The average ratio of lewd words vs. words in a response. The higher the better. |
| Var Score | The lewd word variety score. It counts how many different lewd words occur in all ERP responses |

Updated: 2023-11-04 19:17:13 (UTC+01:00) [Changelog](http://ayumi.m8geil.de/ayumi_bench_v3_changelog.html)

| Rank | Name | Size | Q | ALC-IQ3 | ERP3 Score | Var Score |
|-----:|------|-----:|--:|--------:|-----------:|----------:|
| 1 | [ORCA LLaMA QLoRA 70B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230909_70B-Q4_K_M_ORCA_LLaMA_QLoRA.html) | 70B | Q4_K_M | 90.07 | 30.77 | 396 |
| 2 | [Zephyr Alpha 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_2023-10-11_7B-Q5_K_M_Zephyr.html) | 7B | Q5_K_M | 87.50 | 33.03 | 351 |
| 3 | [Sheep Duck LLaMA 2 V1.1 70B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231001_70B-Q4_K_M_Sheep_Duck_LLaMA_2_V1_1.html) | 70B | Q4_K_M | 89.24 | 31.23 | 377 |
| 4 | [U Amethyst 20B](http://ayumi.m8geil.de/results_v3/model_resp_DL_2023-09-27_20B-Q5_K_M_U_Amethyst.html) | 20B | Q5_K_M | 88.86 | 30.95 | 455 |
| 5 | [Xwin LM V0.1 70B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230921_70B-Q4_K_M_Xwin_LM_V0_1.html) | 70B | Q4_K_M | 88.54 | 31.02 | 362 |
| 6 | [PsyMedRP V1 20B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231008_20B-Q5_K_M_PsyMedRP_V1.html) | 20B | Q5_K_M | 88.48 | 30.59 | 440 |
| 7 | [Nethena 20B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231029_20B-Q5_K_M_Nethena.html) | 20B | Q5_K_M | 86.35 | 32.60 | 400 |
| 8 | [StellarBright 70B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231015_70B-Q4_K_M_StellarBright.html) | 70B | Q4_K_M | 88.56 | 30.36 | 404 |
| 9 | [Dolphin 2.1 Mistral 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231011_7B-Q5_K_M_Dolphin_2_1_Mistral.html) | 7B | Q5_K_M | 86.69 | 31.74 | 359 |
| 10 | [Sheep Duck LLaMA 2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231008_13B-Q5_K_M_Sheep_Duck_LLaMA_2.html) | 13B | Q5_K_M | 87.83 | 30.39 | 400 |
| 11 | [SciPhi Self RAG Mistral 32k 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231101_7B-Q5_K_M_SciPhi_Self_RAG_Mistral_32k.html) | 7B | Q5_K_M | 88.00 | 30.06 | 263 |
| 12 | [Airoboros L2 2.2.1 70B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230921_70B-Q4_K_M_Airoboros_L2_2_2_1.html) | 70B | Q4_K_M | 87.47 | 30.50 | 346 |
| 13 | [Zephyr Beta 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_2023-10-27_7B-Q5_K_M_Zephyr_Beta.html) | 7B | Q5_K_M | 85.83 | 31.92 | 323 |
| 14 | [Emerhyst 20B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230928_20B-Q5_K_M_Emerhyst.html) | 20B | Q5_K_M | 88.33 | 29.20 | 423 |
| 15 | [Nethena MLewd Xwin 23B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231031_23B-Q5_K_M_Nethena_MLewd_Xwin.html) | 23B | Q5_K_M | 83.30 | 33.98 | 405 |
| 16 | [Unholy v1.1 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230913_13B-Q5_K_M_Unholy_v1_1.html) | 13B | Q5_K_M | 86.20 | 30.72 | 318 |
| 17 | [StableBeluga 2 70B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230906_70B-Q4_K_M_StableBeluga_2.html) | 70B | Q4_K_M | 87.51 | 29.39 | 391 |
| 18 | [OpenHermes 2.5 Mistral 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231102_7B-Q5_K_M_OpenHermes_2_5_Mistral.html) | 7B | Q5_K_M | 87.65 | 29.17 | 337 |
| 19 | [Synatra V0.3 RP 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231030_7B-Q4_K_M_Synatra_V0_3_RP.html) | 7B | Q4_K_M | 82.69 | 34.04 | 425 |
| 20 | [LLaMA-2 Chat AYT 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230913_13B-Q5_K_M_LLaMA-2_Chat_AYT.html) | 13B | Q5_K_M | 89.88 | 26.82 | 364 |
| 21 | [LLaMA-2 Chat AYB 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231008_13B-Q5_K_M_LLaMA-2_Chat_AYB.html) | 13B | Q5_K_M | 87.83 | 28.75 | 355 |
| 22 | [Thespis Mistral V0.5 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231103_7B-Q5_K_M_Thespis_Mistral_V0_5.html) | 7B | Q5_K_M | 82.51 | 33.89 | 318 |
| 23 | [LLaMA-2 Ensemble v6 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230912_13B-Q5_K_M_LLaMA-2_Ensemble_v6.html) | 13B | Q5_K_M | 86.93 | 29.25 | 482 |
| 24 | [Airoboros M 3.1.2 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_2023-10-19_7B-Q5_K_M_Airoboros_M_3_1_2.html) | 7B | Q5_K_M | 84.02 | 32.09 | 270 |
| 25 | [Stheno 1.8 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230927_13B-Q5_K_M_Stheno_1_8.html) | 13B | Q5_K_M | 84.72 | 31.27 | 390 |
| 26 | [Utopia 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231102_13B-Q5_K_M_Utopia.html) | 13B | Q5_K_M | 85.05 | 30.85 | 439 |
| 27 | [ZephRP M 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231012_7B-Q5_K_M_ZephRP_M.html) | 7B | Q5_K_M | 85.79 | 30.01 | 397 |
| 28 | [lzlv 70B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231025_70B-Q4_K_M_lzlv.html) | 70B | Q4_K_M | 86.02 | 29.61 | 321 |
| 29 | [MLewdBoros SuperCOT 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230912_13B-Q5_K_M_MLewdBoros_SuperCOT.html) | 13B | Q5_K_M | 82.63 | 32.96 | 366 |
| 30 | [SlimOpenOrca Mistral 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231018_7B-Q5_K_M_SlimOpenOrca_Mistral.html) | 7B | Q5_K_M | 88.49 | 27.02 | 403 |
| 31 | [Mistral OpenOrca 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231002_7B-Q5_K_M_Mistral_OpenOrca.html) | 7B | Q5_K_M | 86.48 | 28.91 | 381 |
| 32 | [Dolphin 2.2.1 Mistral 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231030_7B-Q5_K_M_Dolphin_2_2_1_Mistral.html) | 7B | Q5_K_M | 85.99 | 29.25 | 317 |
| 33 | [OpenRP SuperCOT 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230912_13B-Q5_K_M_OpenRP_SuperCOT.html) | 13B | Q5_K_M | 84.47 | 30.59 | 336 |
| 34 | [Airoboros L2 2.1 70B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230906_70B-Q4_K_M_Airoboros_L2_2_1.html) | 70B | Q4_K_M | 83.50 | 31.26 | 389 |
| 35 | [OpenHermes 2 Mistral 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_2023-10-14_7B-Q5_K_M_OpenHermes_2_Mistral.html) | 7B | Q5_K_M | 84.16 | 30.08 | 312 |
| 36 | [Athena v3 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230927_13B-Q5_K_M_Athena_v3.html) | 13B | Q5_K_M | 81.45 | 32.76 | 366 |
| 37 | [Thorns 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230907_13B-Q5_K_M_Thorns.html) | 13B | Q5_K_M | 79.08 | 35.10 | 268 |
| 38 | [Unholy v1 12L 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230910_13B-Q5_K_M_Unholy_v1_12L.html) | 13B | Q5_K_M | 82.39 | 31.21 | 385 |
| 39 | [Airoboros L2 2.1 Creative 70B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230831_70B-Q4_K_M_Airoboros_L2_2_1_Creative.html) | 70B | Q4_K_M | 83.27 | 30.23 | 379 |
| 40 | [GodziLLa 2 70B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230906_70B-Q4_K_M_GodziLLa_2.html) | 70B | Q4_K_M | 83.19 | 30.29 | 404 |
| 41 | [Athnete 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231029_13B-Q5_K_M_Athnete.html) | 13B | Q5_K_M | 81.91 | 31.53 | 403 |
| 42 | [Mistral Dolphin 2.1 LIMA0.5 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_2023-10-28_7B-Q5_K_M_Mistral_Dolphin_2_1_LIMA0_5.html) | 7B | Q5_K_M | 85.34 | 28.02 | 362 |
| 43 | [Mistral AirOmniMix 11B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231014_11B-Q6_K_Mistral_AirOmniMix.html) | 11B | Q6_K | 83.39 | 29.89 | 337 |
| 44 | [Nous Hermes 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230902_13B-Q5_K_M_Nous_Hermes.html) | 13B | Q5_K_M | 81.22 | 31.81 | 257 |
| 45 | [Mistral Phibrarian 32K 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_2023-10-17_7B-Q5_K_M_Mistral_Phibrarian_32K.html) | 7B | Q5_K_M | 83.54 | 29.50 | 307 |
| 46 | [Synthia V1.1 70B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230831_70B-Q4_K_M_Synthia_V1_1.html) | 70B | Q4_K_M | 82.94 | 30.00 | 359 |
| 47 | [Airoboros Mistral 2.2 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231003_7B-Q5_K_M_Airoboros_Mistral_2_2.html) | 7B | Q5_K_M | 80.23 | 32.39 | 290 |
| 48 | [MLewd Chat 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230916_13B-Q5_K_M_MLewd_Chat.html) | 13B | Q5_K_M | 83.69 | 28.88 | 336 |
| 49 | [WizardLM V1.0 70B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230906_70B-Q4_K_M_WizardLM_V1_0.html) | 70B | Q4_K_M | 85.99 | 26.56 | 269 |
| 50 | [Mistral SciPhi 32k 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231022_7B-Q5_K_M_Mistral_SciPhi_32k.html) | 7B | Q5_K_M | 83.43 | 29.02 | 325 |
| 51 | [CollectiveCognition V1.1 Mistral 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231005_7B-Q5_K_M_CollectiveCognition_V1_1_Mistral.html) | 7B | Q5_K_M | 85.28 | 27.02 | 246 |
| 52 | [Echidna V0.1 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231022_13B-Q5_K_M_Echidna_V0_1.html) | 13B | Q5_K_M | 80.41 | 31.80 | 400 |
| 53 | [Dolphin 2.2.1 AshhLimaRP Mistral 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231104_7B-Q5_K_M_Dolphin_2_2_1_AshhLimaRP_Mistral.html) | 7B | Q5_K_M | 85.00 | 27.09 | 355 |
| 54 | [Mistral Trismegistus 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231007_7B-Q5_K_M_Mistral_Trismegistus.html) | 7B | Q5_K_M | 79.05 | 32.91 | 191 |
| 55 | [Athena V4 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231008_B-Q5_K_M_Athena_V4.html) | 13B | Q5_K_M | 80.98 | 30.95 | 411 |
| 56 | [Vigostral Chat 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231024_7B-Q5_K_M_Vigostral_Chat.html) | 7B | Q5_K_M | 81.38 | 30.52 | 314 |
| 57 | [Nethena 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231029_13B-Q5_K_M_Nethena.html) | 13B | Q5_K_M | 80.40 | 31.44 | 404 |
| 58 | [UndiMix v2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230901_13B-Q5_K_M_UndiMix_v2.html) | 13B | Q5_K_M | 79.50 | 32.22 | 316 |
| 59 | [Airoboros L2 GPT4 1.4.1 70B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230906_70B-Q4_K_M_Airoboros_L2_GPT4_1_4_1.html) | 70B | Q4_K_M | 82.93 | 28.47 | 376 |
| 60 | [UndiMix v4 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230913_13B-Q5_K_M_UndiMix_v4.html) | 13B | Q5_K_M | 79.02 | 32.24 | 332 |
| 61 | [ReMM v2 Kimiko v2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230912_13B-Q5_K_M_ReMM_v2_Kimiko_v2.html) | 13B | Q5_K_M | 81.60 | 29.65 | 365 |
| 62 | [HornyEchidna V0.1 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_2023-10-24_13B-Q5_K_M_HornyEchidna_V0_1.html) | 13B | Q5_K_M | 80.48 | 30.74 | 405 |
| 63 | [Synthia v1.3 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230929_7B-Q5_K_M_Synthia_v1_3.html) | 7B | Q5_K_M | 78.71 | 32.46 | 333 |
| 64 | [MistralLite 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_2023-10-19_7B-Q5_K_M_MistralLite.html) | 7B | Q5_K_M | 82.20 | 28.93 | 356 |
| 65 | [MistRP Airoboros 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_2023-10-27_7B-Q5_K_M_MistRP_Airoboros.html) | 7B | Q5_K_M | 80.48 | 30.61 | 254 |
| 66 | [MXLewdMini 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230922_13B-Q5_K_M_MXLewdMini.html) | 13B | Q5_K_M | 79.11 | 31.94 | 347 |
| 67 | [ReMM v2.2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230922_13B-Q5_K_M_ReMM_v2_2.html) | 13B | Q5_K_M | 79.63 | 31.11 | 327 |
| 68 | [StableBeluga 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230906_13B-Q5_K_M_StableBeluga.html) | 13B | Q5_K_M | 82.65 | 28.02 | 350 |
| 69 | [ReMM v2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230909_13B-Q5_K_M_ReMM_v2.html) | 13B | Q5_K_M | 78.69 | 31.74 | 372 |
| 70 | [Nous Hermes LLaMA-2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230908_13B-Q5_K_M_Nous_Hermes_LLaMA-2.html) | 13B | Q5_K_M | 79.25 | 31.07 | 239 |
| 71 | [Athena v2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230923_13B-Q5_K_M_Athena_v2.html) | 13B | Q5_K_M | 79.15 | 31.10 | 392 |
| 72 | [MistralMakise Merged 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231021_13B-Q5_K_M_MistralMakise_Merged.html) | 13B | Q5_K_M | 80.73 | 29.43 | 425 |
| 73 | [Lewd Sydney 20B](http://ayumi.m8geil.de/results_v3/model_resp_DL_2023-10-27_20B-Q4_K_S_Lewd_Sydney.html) | 20B | Q4_K_S | 82.58 | 27.49 | 320 |
| 74 | [UndiMix V3 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231011_13B-Q5_K_M_UndiMix_V3.html) | 13B | Q5_K_M | 78.25 | 31.68 | 356 |
| 75 | [Nete 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231026_13B-Q5_K_M_Nete.html) | 13B | Q5_K_M | 79.74 | 30.16 | 434 |
| 76 | [UndiMix V4 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231006_13B-Q5_K_M_UndiMix_V4.html) | 13B | Q5_K_M | 79.01 | 30.81 | 319 |
| 77 | [MLewd v2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230906_13B-Q5_K_M_MLewd_v2.html) | 13B | Q5_K_M | 77.99 | 31.81 | 395 |
| 78 | [Echidna V0.2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231026_13B-Q5_K_M_Echidna_V0_2.html) | 13B | Q5_K_M | 78.56 | 31.22 | 366 |
| 79 | [Unholy v1 10L 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230910_13B-Q5_K_M_Unholy_v1_10L.html) | 13B | Q5_K_M | 80.59 | 29.19 | 355 |
| 80 | [BerrySauce 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230914_13B-Q5_K_M_BerrySauce.html) | 13B | Q5_K_M | 77.09 | 32.42 | 394 |
| 81 | [SynthIA V1.5 70B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231025_70B-Q4_K_M_SynthIA_V1_5.html) | 70B | Q4_K_M | 79.54 | 29.92 | 356 |
| 82 | [Camel Platypus2 70B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230906_70B-Q4_K_M_Camel_Platypus2.html) | 70B | Q4_K_M | 82.90 | 26.49 | 337 |
| 83 | [L2 TheSpurral M2.2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231103_13B-Q5_K_M_L2_TheSpurral_M2_2.html) | 13B | Q5_K_M | 78.49 | 30.83 | 369 |
| 84 | [ReMM S Kimiko v2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230907_13B-Q5_K_M_ReMM_S_Kimiko_v2.html) | 13B | Q5_K_M | 76.27 | 32.97 | 459 |
| 85 | [ReMM Mistral 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231001_13B-Q5_K_M_ReMM_Mistral.html) | 13B | Q5_K_M | 80.58 | 28.63 | 396 |
| 86 | [ReMM v2.1 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230912_13B-Q5_K_M_ReMM_v2_1.html) | 13B | Q5_K_M | 77.29 | 31.89 | 322 |
| 87 | [Mistralic 1 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231004_7B-Q5_K_M_Mistralic_1.html) | 7B | Q5_K_M | 80.67 | 28.41 | 335 |
| 88 | [LLaMA 2 Tiefighter 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231022_13B-Q5_K_M_LLaMA_2_Tiefighter.html) | 13B | Q5_K_M | 78.89 | 30.07 | 393 |
| 89 | [AppleSauce 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230910_13B-Q5_K_M_AppleSauce.html) | 13B | Q5_K_M | 76.47 | 32.42 | 383 |
| 90 | [UndiMix v3 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230907_13B-Q5_K_M_UndiMix_v3.html) | 13B | Q5_K_M | 78.32 | 30.55 | 368 |
| 91 | [MythoMax Kimiko Mix 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230829_13B-Q5_K_M_MythoMax_Kimiko_Mix.html) | 13B | Q5_K_M | 79.60 | 29.16 | 385 |
| 92 | [Echidna V0.3 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231027_13B-Q5_K_M_Echidna_V0_3.html) | 13B | Q5_K_M | 78.52 | 30.19 | 368 |
| 93 | [ReMM v2 Variant 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230909_13B-Q5_K_M_ReMM_v2_Variant.html) | 13B | Q5_K_M | 78.05 | 30.61 | 304 |
| 94 | [UndiMix v1 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230831_13B-Q5_K_M_UndiMix_v1.html) | 13B | Q5_K_M | 77.78 | 30.73 | 307 |
| 95 | [MythoMax Kimiko V2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230831_13B-Q5_K_M_MythoMax_Kimiko_V2.html) | 13B | Q5_K_M | 79.53 | 28.94 | 381 |
| 96 | [Mistral SynthIAirOmniMix 11B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231014_11B-Q5_K_M_Mistral_SynthIAirOmniMix.html) | 11B | Q5_K_M | 79.14 | 29.18 | 310 |
| 97 | [MLewd v2-2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230908_13B-Q5_K_M_MLewd_v2-2.html) | 13B | Q5_K_M | 76.26 | 31.85 | 376 |
| 98 | [Stheno Inverted 1.2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230906_13B-Q5_K_M_Stheno_Inverted_1_2.html) | 13B | Q5_K_M | 76.95 | 30.98 | 418 |
| 99 | [LLaMA-2 LoRA Assemble 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230914_7B-Q5_K_M_LLaMA-2_LoRA_Assemble.html) | 7B | Q5_K_M | 77.82 | 30.07 | 287 |
| 100 | [Guanaco 65B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230920_65B-Q4_K_M_Guanaco.html) | 65B | Q4_K_M | 78.85 | 29.00 | 330 |
| 101 | [L2 TheSpurral M2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231029_13B-Q5_K_S_L2_TheSpurral_M2.html) | 13B | Q5_K_S | 76.32 | 31.43 | 371 |
| 102 | [Xwin LM V0.2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_2023-10-15_13B-Q5_K_M_Xwin_LM_V0_2.html) | 13B | Q5_K_M | 79.27 | 28.46 | 355 |
| 103 | [Slerpeno 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230908_13B-Q5_K_M_Slerpeno.html) | 13B | Q5_K_M | 74.74 | 32.92 | 330 |
| 104 | [Xwin MLewd V0.2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_2023-10-15_13B-Q5_K_M_Xwin_MLewd_V0_2.html) | 13B | Q5_K_M | 78.53 | 29.06 | 412 |
| 105 | [Airoboros 2.1 33B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230901_33B-Q4_K_M_Airoboros_2_1.html) | 33B | Q4_K_M | 75.62 | 31.82 | 377 |
| 106 | [Uncensored Jordan 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231030_13B-Q5_K_M_Uncensored_Jordan.html) | 13B | Q5_K_M | 79.92 | 27.42 | 229 |
| 107 | [ReMM v1 LRPSGPT 2Char 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230913_13B-Q5_K_M_ReMM_v1_LRPSGPT_2Char.html) | 13B | Q5_K_M | 74.89 | 32.40 | 373 |
| 108 | [MLewdBoros 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230909_13B-Q5_K_M_MLewdBoros.html) | 13B | Q5_K_M | 75.78 | 31.52 | 407 |
| 109 | [Augmental V1.50 A 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231029_13B-Q5_K_M_Augmental_V1_50_A.html) | 13B | Q5_K_M | 77.43 | 29.72 | 375 |
| 110 | [MLewd V2-1 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230906_13B-Q5_K_M_MLewd_V2-1.html) | 13B | Q5_K_M | 76.63 | 30.48 | 422 |
| 111 | [Magpie 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230916_13B-Q5_K_M_Magpie.html) | 13B | Q5_K_M | 78.02 | 29.06 | 350 |
| 112 | [Dans TotSirocco 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231004_7B-Q5_K_M_Dans_TotSirocco.html) | 7B | Q5_K_M | 79.47 | 27.54 | 283 |
| 113 | [Amethyst 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230924_13B-Q5_K_M_Amethyst.html) | 13B | Q5_K_M | 80.06 | 26.90 | 430 |
| 114 | [Mistral RP 0.1 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230929_7B-Q5_K_M_Mistral_RP_0_1.html) | 7B | Q5_K_M | 77.86 | 29.05 | 349 |
| 115 | [Frank Uncensored 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230919_13B-Q5_K_M_Frank_Uncensored.html) | 13B | Q5_K_M | 76.04 | 30.81 | 228 |
| 116 | [Stheno Inverted 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230901_13B-Q5_K_M_Stheno_Inverted.html) | 13B | Q5_K_M | 77.19 | 29.65 | 393 |
| 117 | [Airochronos 33B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230920_33B-Q4_K_M_Airochronos.html) | 33B | Q4_K_M | 75.00 | 31.83 | 342 |
| 118 | [ZettaPi 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230921_13B-Q5_K_M_ZettaPi.html) | 13B | Q5_K_M | 78.36 | 28.46 | 382 |
| 119 | [MLewd 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230904_13B-Q5_K_M_MLewd.html) | 13B | Q5_K_M | 74.71 | 32.08 | 348 |
| 120 | [MythoLogic 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230905_13B-Q5_K_M_MythoLogic.html) | 13B | Q5_K_M | 75.22 | 31.57 | 263 |
| 121 | [LLaMA 2 Arguments 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231005_7B-Q5_K_M_LLaMA_2_Arguments.html) | 7B | Q5_K_M | 76.80 | 29.98 | 218 |
| 122 | [Tulpar Limarp 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230827_7B-Q5_K_M_Tulpar_Limarp.html) | 7B | Q5_K_M | 78.48 | 28.28 | 364 |
| 123 | [ReMM SLERP 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230904_13B-Q5_K_M_ReMM_SLERP.html) | 13B | Q5_K_M | 77.75 | 28.95 | 385 |
| 124 | [Huginn v1.2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230905_13B-Q5_K_M_Huginn_v1_2.html) | 13B | Q5_K_M | 77.75 | 28.95 | 385 |
| 125 | [MythoMax 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230905_13B-Q5_K_M_MythoMax.html) | 13B | Q5_K_M | 77.75 | 28.95 | 385 |
| 126 | [LLaMA-2 Silverlin. Verilog 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230913_7B-Q4_K_M_LLaMA-2_Silverlin__Verilog.html) | 7B | Q4_K_M | 77.03 | 29.56 | 186 |
| 127 | [Pygmalion 2 SuperCOT2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230914_13B-Q5_K_M_Pygmalion_2_SuperCOT2.html) | 13B | Q5_K_M | 75.76 | 30.81 | 217 |
| 128 | [Arithmo Mistral 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_2023-10-20_7B-Q5_K_M_Arithmo_Mistral.html) | 7B | Q5_K_M | 77.02 | 29.47 | 271 |
| 129 | [AgentLM 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231021_7B-Q5_K_M_AgentLM.html) | 7B | Q5_K_M | 77.02 | 29.45 | 190 |
| 130 | [ReMM 0.65 SLERP 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230909_13B-Q5_K_M_ReMM_0_65_SLERP.html) | 13B | Q5_K_M | 76.25 | 30.18 | 342 |
| 131 | [Airoboros L2 2.2.1 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230923_13B-Q5_K_M_Airoboros_L2_2_2_1.html) | 13B | Q5_K_M | 75.19 | 31.09 | 335 |
| 132 | [Airoboros GPT4 2.0 LLaMA-2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230908_13B-Q5_K_M_Airoboros_GPT4_2_0_LLaMA-2.html) | 13B | Q5_K_M | 73.61 | 32.58 | 274 |
| 133 | [Chronoboros 33B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230920_33B-Q4_K_M_Chronoboros.html) | 33B | Q4_K_M | 74.93 | 31.21 | 360 |
| 134 | [MLewd V2-1 015 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230906_13B-Q4_K_S_MLewd_V2-1_015.html) | 13B | Q4_K_S | 75.96 | 30.13 | 387 |
| 135 | [MegaMix T1 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230930_13B-Q5_K_M_MegaMix_T1.html) | 13B | Q5_K_M | 76.36 | 29.70 | 298 |
| 136 | [MythoMix 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230905_13B-Q5_K_M_MythoMix.html) | 13B | Q5_K_M | 76.34 | 29.51 | 384 |
| 137 | [Amethyst Mistral 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231001_13B-Q4_K_S_Amethyst_Mistral.html) | 13B | Q4_K_S | 79.25 | 26.53 | 419 |
| 138 | [MegaMix A1 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230930_13B-Q5_K_M_MegaMix_A1.html) | 13B | Q5_K_M | 76.53 | 29.22 | 309 |
| 139 | [Pygmalion 2 SuperCOT 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230910_13B-Q5_K_M_Pygmalion_2_SuperCOT.html) | 13B | Q5_K_M | 77.70 | 28.03 | 255 |
| 140 | [Augmental V1.50 B 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231029_13B-Q5_K_M_Augmental_V1_50_B.html) | 13B | Q5_K_M | 76.91 | 28.75 | 359 |
| 141 | [Airoboros GPT4 2.0 LLaMA-2 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230908_7B-Q5_K_M_Airoboros_GPT4_2_0_LLaMA-2.html) | 7B | Q5_K_M | 73.66 | 31.95 | 220 |
| 142 | [MLewdBoros LRPSGPT 2Char 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230913_13B-Q5_K_M_MLewdBoros_LRPSGPT_2Char.html) | 13B | Q5_K_M | 76.78 | 28.83 | 382 |
| 143 | [OpenRP 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230911_13B-Q5_K_M_OpenRP.html) | 13B | Q5_K_M | 77.04 | 28.42 | 411 |
| 144 | [Inkbot 4k 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230921_13B-Q4_K_M_Inkbot_4k.html) | 13B | Q4_K_M | 77.20 | 28.14 | 367 |
| 145 | [Athena v1 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230830_13B-Q5_K_M_Athena_v1.html) | 13B | Q5_K_M | 74.58 | 30.74 | 352 |
| 146 | [Airoboros L2 3.0 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231005_13B-Q5_K_M_Airoboros_L2_3_0.html) | 13B | Q5_K_M | 75.97 | 29.32 | 345 |
| 147 | [Marcoroni 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230911_7B-Q5_K_M_Marcoroni.html) | 7B | Q5_K_M | 75.69 | 29.34 | 301 |
| 148 | [Mistral Claude Chat 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230927_7B-Q5_K_M_Mistral_Claude_Chat.html) | 7B | Q5_K_M | 74.83 | 30.14 | 233 |
| 149 | [Zaraxls 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230827_7B-Q5_K_M_Zaraxls.html) | 7B | Q5_K_M | 74.56 | 30.29 | 410 |
| 150 | [MythoMakiseMerged 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231001_13B-Q5_K_M_MythoMakiseMerged.html) | 13B | Q5_K_M | 77.02 | 27.77 | 351 |
| 151 | [Chronos V2 70B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230906_70B-Q4_K_M_Chronos_V2.html) | 70B | Q4_K_M | 76.67 | 27.76 | 362 |
| 152 | [LlongOrca 16K 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230911_13B-Q5_K_M_LlongOrca_16K.html) | 13B | Q5_K_M | 78.47 | 25.83 | 368 |
| 153 | [Platypus 2 70B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230906_70B-Q4_K_M_Platypus_2.html) | 70B | Q4_K_M | 78.04 | 26.06 | 330 |
| 154 | [ReMM PIPPA 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230904_13B-Q5_K_M_ReMM_PIPPA.html) | 13B | Q5_K_M | 74.73 | 29.34 | 410 |
| 155 | [Stheno 1.3 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230908_13B-Q5_K_M_Stheno_1_3.html) | 13B | Q5_K_M | 72.94 | 31.12 | 457 |
| 156 | [Emerhyst 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230928_13B-Q5_K_M_Emerhyst.html) | 13B | Q5_K_M | 78.44 | 25.58 | 404 |
| 157 | [ReMM Lion 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230906_13B-Q5_K_M_ReMM_Lion.html) | 13B | Q5_K_M | 76.02 | 27.85 | 363 |
| 158 | [OpenBuddy Mistral v13 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_2023-10-16_7B-Q5_K_M_OpenBuddy_Mistral_v13.html) | 7B | Q5_K_M | 72.53 | 31.32 | 249 |
| 159 | [Samantha Mistral 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230930_7B-Q5_K_M_Samantha_Mistral.html) | 7B | Q5_K_M | 76.16 | 27.49 | 251 |
| 160 | [ReMM 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230901_13B-Q5_K_M_ReMM.html) | 13B | Q5_K_M | 74.55 | 29.07 | 416 |
| 161 | [Mythalion 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230905_13B-Q5_K_M_Mythalion.html) | 13B | Q5_K_M | 74.39 | 29.05 | 332 |
| 162 | [LLaMA 2 Chat 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230904_7B-Q5_K_M_LLaMA_2_Chat.html) | 7B | Q5_K_M | 74.44 | 28.89 | 203 |
| 163 | [Teknium OpenHermes 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230908_13B-Q5_K_S_Teknium_OpenHermes.html) | 13B | Q5_K_S | 71.81 | 31.34 | 275 |
| 164 | [Nous Capybara V1.9 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231029_7B-Q5_K_M_Nous_Capybara_V1_9.html) | 7B | Q5_K_M | 73.07 | 29.94 | 316 |
| 165 | [Yarn Mistral 64k 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231102_7B-Q5_K_M_Yarn_Mistral_64k.html) | 7B | Q5_K_M | 75.03 | 27.89 | 331 |
| 166 | [Thespis V0.5 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231030_13B-Q5_K_M_Thespis_V0_5.html) | 13B | Q5_K_M | 72.61 | 30.23 | 276 |
| 167 | [MLewd V2-1 050 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230906_13B-Q4_K_S_MLewd_V2-1_050.html) | 13B | Q4_K_S | 74.13 | 28.69 | 381 |
| 168 | [PsyMedRP V1 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231018_13B-Q5_K_M_PsyMedRP_V1.html) | 13B | Q5_K_M | 76.76 | 26.07 | 404 |
| 169 | [Vicuna v1.5 16K 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230905_7B-Q5_K_M_Vicuna_v1_5_16K.html) | 7B | Q5_K_M | 71.41 | 31.35 | 234 |
| 170 | [GradientPutri MegaMix S1 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230929_13B-Q5_K_S_GradientPutri_MegaMix_S1.html) | 13B | Q5_K_S | 73.27 | 29.39 | 312 |
| 171 | [Stheno Chat 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230921_13B-Q5_K_M_Stheno_Chat.html) | 13B | Q5_K_M | 74.94 | 27.64 | 268 |
| 172 | [LLaMA 2 Chat 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230904_13B-Q5_K_M_LLaMA_2_Chat.html) | 13B | Q5_K_M | 74.29 | 27.55 | 250 |
| 173 | [L2 TheSpurral V2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231027_13B-Q5_K_S_L2_TheSpurral_V2.html) | 13B | Q5_K_S | 71.22 | 30.53 | 345 |
| 174 | [Vicuna v1.5 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230905_7B-Q5_K_M_Vicuna_v1_5.html) | 7B | Q5_K_M | 72.72 | 29.03 | 226 |
| 175 | [Mistral v0.1 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230927_7B-Q5_K_M_Mistral_v0_1.html) | 7B | Q5_K_M | 72.67 | 28.81 | 298 |
| 176 | [Yarn Mistral 128k 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231102_7B-Q5_K_M_Yarn_Mistral_128k.html) | 7B | Q5_K_M | 75.29 | 26.17 | 341 |
| 177 | [Mistral ClaudeLimaRP v3 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230928_7B-Q5_K_M_Mistral_ClaudeLimaRP_v3.html) | 7B | Q5_K_M | 73.78 | 27.59 | 375 |
| 178 | [AgentLM 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231020_13B-Q5_K_M_AgentLM.html) | 13B | Q5_K_M | 72.63 | 28.64 | 206 |
| 179 | [AstraMix 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230831_7B-Q5_K_M_AstraMix.html) | 7B | Q5_K_M | 72.52 | 28.74 | 359 |
| 180 | [UltraLM V2.0 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231001_13B-Q5_K_M_UltraLM_V2_0.html) | 13B | Q5_K_M | 71.92 | 29.28 | 282 |
| 181 | [WizardLM v1.2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230904_13B-Q4_0_WizardLM_v1_2.html) | 13B | Q4_0 | 75.81 | 25.28 | 300 |
| 182 | [Medusa 1.1 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230906_7B-Q5_K_M_Medusa_1_1.html) | 7B | Q5_K_M | 71.06 | 29.98 | 284 |
| 183 | [Xwin LM V0.1 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230921_7B-Q5_K_M_Xwin_LM_V0_1.html) | 7B | Q5_K_M | 65.09 | 35.85 | 214 |
| 184 | [TerraMix 16K 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230906_13B-Q5_K_M_TerraMix_16K.html) | 13B | Q5_K_M | 74.97 | 25.92 | 352 |
| 185 | [Airoboros Creative lmoe 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230826_13B-Q5_K_M_Airoboros_Creative_lmoe.html) | 13B | Q5_K_M | 71.22 | 29.61 | 382 |
| 186 | [Chronos Hermes v2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230908_13B-Q5_K_M_Chronos_Hermes_v2.html) | 13B | Q5_K_M | 72.39 | 28.38 | 332 |
| 187 | [LosslessMegaCoder Mini 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230908_7B-Q5_K_M_LosslessMegaCoder_Mini.html) | 7B | Q5_K_M | 69.96 | 30.37 | 263 |
| 188 | [Thespurral V1 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_2023-10-25_13B-Q5_K_M_Thespurral_V1.html) | 13B | Q5_K_M | 69.55 | 30.73 | 332 |
| 189 | [Pygmaltion 2 SuperCOT weighted 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230916_13B-Q5_K_M_Pygmaltion_2_SuperCOT_weighted.html) | 13B | Q5_K_M | 70.92 | 29.29 | 275 |
| 190 | [Airoboros 2.1 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230829_13B-Q5_K_M_Airoboros_2_1.html) | 13B | Q5_K_M | 71.16 | 28.94 | 391 |
| 191 | [Luna AI LLaMA-2 Uncensored 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230906_7B-Q5_K_M_Luna_AI_LLaMA-2_Uncensored.html) | 7B | Q5_K_M | 67.13 | 32.85 | 245 |
| 192 | [StableBeluga 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230905_7B-Q5_K_M_StableBeluga.html) | 7B | Q5_K_M | 73.27 | 26.66 | 291 |
| 193 | [Kimiko Mistral 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230930_7B-Q5_K_M_Kimiko_Mistral.html) | 7B | Q5_K_M | 74.18 | 25.68 | 317 |
| 194 | [Zarafusionex 1.1 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230905_7B-Q5_K_M_Zarafusionex_1_1.html) | 7B | Q5_K_M | 71.08 | 28.61 | 365 |
| 195 | [Airoboros GPT4 m2.0 LLaMA-2 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230908_7B-Q5_K_M_Airoboros_GPT4_m2_0_LLaMA-2.html) | 7B | Q5_K_M | 69.69 | 30.00 | 212 |
| 196 | [Airoboros 2.2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230912_13B-Q5_K_M_Airoboros_2_2.html) | 13B | Q5_K_M | 70.45 | 28.91 | 378 |
| 197 | [Basilisk V0.2 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231102_7B-Q5_K_M_Basilisk_V0_2.html) | 7B | Q5_K_M | 68.55 | 30.59 | 287 |
| 198 | [Spicyboros 2.2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230910_13B-Q4_K_M_Spicyboros_2_2.html) | 13B | Q4_K_M | 70.58 | 28.50 | 389 |
| 199 | [MegaMix S1 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230930_13B-Q5_K_M_MegaMix_S1.html) | 13B | Q5_K_M | 72.97 | 25.83 | 296 |
| 200 | [Zarablend 1.1 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230826_7B-Q5_K_M_Zarablend_1_1.html) | 7B | Q5_K_M | 65.62 | 33.09 | 319 |
| 201 | [AshhLimaRP Mistral 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_2023-10-27_7B-Q5_K_M_AshhLimaRP_Mistral.html) | 7B | Q5_K_M | 74.07 | 24.68 | 380 |
| 202 | [ELYZA Jp LLaMA-2 Instruct 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230829_7B-Q5_K_M_ELYZA_Jp_LLaMA-2_Instruct.html) | 7B | Q5_K_M | 69.42 | 29.17 | 164 |
| 203 | [PetrolLM 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230928_7B-Q5_K_M_PetrolLM.html) | 7B | Q5_K_M | 74.81 | 23.71 | 313 |
| 204 | [Spicyboros 2.2 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230908_7B-Q5_K_M_Spicyboros_2_2.html) | 7B | Q5_K_M | 66.38 | 31.94 | 311 |
| 205 | [Pygmalion 2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230905_13B-Q5_K_M_Pygmalion_2.html) | 13B | Q5_K_M | 69.17 | 29.05 | 284 |
| 206 | [Fireflx v1.2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230905_13B-Q5_K_M_Fireflx_v1_2.html) | 13B | Q5_K_M | 69.25 | 28.70 | 285 |
| 207 | [Thespis V0.4 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231022_13B-Q5_K_M_Thespis_V0_4.html) | 13B | Q5_K_M | 69.86 | 27.96 | 264 |
| 208 | [MedLLama 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230902_7B-Q5_K_M_MedLLama.html) | 7B | Q5_K_M | 70.60 | 27.18 | 219 |
| 209 | [Augmental 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_2023-10-24_13B-Q5_K_M_Augmental.html) | 13B | Q5_K_M | 71.20 | 26.50 | 368 |
| 210 | [Dans AdventurousWinds 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231004_7B-Q5_K_M_Dans_AdventurousWinds.html) | 7B | Q5_K_M | 72.38 | 24.93 | 298 |
| 211 | [Airoboros GPT4 1.4.1 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230905_13B-Q5_K_M_Airoboros_GPT4_1_4_1.html) | 13B | Q5_K_M | 69.09 | 28.15 | 316 |
| 212 | [Leo Hessianai Chat 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230928_7B-Q5_K_M_Leo_Hessianai_Chat.html) | 7B | Q5_K_M | 67.73 | 29.42 | 244 |
| 213 | [Ziya Coding V1.0 34B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231009_34B-Q4_K_M_Ziya_Coding_V1_0.html) | 34B | Q4_K_M | 75.44 | 21.74 | 246 |
| 214 | [Airoboros L2 3.0 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231005_7B-Q5_K_M_Airoboros_L2_3_0.html) | 7B | Q5_K_M | 67.75 | 29.36 | 323 |
| 215 | [Mistral Instruct v0.1 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230927_7B-Q5_K_M_Mistral_Instruct_v0_1.html) | 7B | Q5_K_M | 67.07 | 29.81 | 279 |
| 216 | [Nous Capybara 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231002_7B-Q5_K_M_Nous_Capybara.html) | 7B | Q5_K_M | 63.69 | 33.01 | 291 |
| 217 | [Airoboros 2.2 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230912_7B-Q5_K_M_Airoboros_2_2.html) | 7B | Q5_K_M | 67.10 | 29.55 | 284 |
| 218 | [Chronos 33B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230919_33B-Q4_K_M_Chronos.html) | 33B | Q4_K_M | 72.46 | 24.20 | 328 |
| 219 | [Befenghuang Vigogne 2 Chat 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230907_7B-Q5_K_S_Befenghuang_Vigogne_2_Chat.html) | 7B | Q5_K_S | 69.80 | 26.82 | 276 |
| 220 | [CAMEL Combined Data 33B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230920_33B-Q4_K_M_CAMEL_Combined_Data.html) | 33B | Q4_K_M | 67.21 | 29.38 | 277 |
| 221 | [Tulu 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230920_7B-Q5_K_M_Tulu.html) | 7B | Q5_K_M | 75.18 | 21.44 | 185 |
| 222 | [MistRP v1.1 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231001_7B-Q8_0_MistRP_v1_1.html) | 7B | Q8_0 | 70.37 | 26.05 | 279 |
| 223 | [MedLLaMA-2 Chat 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230901_7B-Q5_K_S_MedLLaMA-2_Chat.html) | 7B | Q5_K_S | 69.89 | 26.49 | 273 |
| 224 | [Huginn v3 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230905_13B-Q5_K_M_Huginn_v3.html) | 13B | Q5_K_M | 70.00 | 26.10 | 381 |
| 225 | [Huginn v4 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230829_13B-Q5_K_M_Huginn_v4.html) | 13B | Q5_K_M | 70.00 | 26.10 | 381 |
| 226 | [Huginn v4.5 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230829_13B-Q5_K_M_Huginn_v4_5.html) | 13B | Q5_K_M | 70.00 | 26.10 | 381 |
| 227 | [Thespis V0.3 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231017_13B-Q5_K_M_Thespis_V0_3.html) | 13B | Q5_K_M | 67.59 | 28.43 | 312 |
| 228 | [Dans AdventurousWinds Mk2 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231018_7B-Q5_K_M_Dans_AdventurousWinds_Mk2.html) | 7B | Q5_K_M | 70.35 | 25.55 | 357 |
| 229 | [Kimiko V2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230830_13B-Q5_K_M_Kimiko_V2.html) | 13B | Q5_K_M | 68.02 | 27.73 | 323 |
| 230 | [Airoboros GPT4 1.4.1 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230905_7B-Q5_K_M_Airoboros_GPT4_1_4_1.html) | 7B | Q5_K_M | 63.90 | 31.73 | 268 |
| 231 | [Zarafusionex 1.2 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230829_7B-Q5_K_M_Zarafusionex_1_2.html) | 7B | Q5_K_M | 70.53 | 24.82 | 355 |
| 232 | [Zarablend 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230905_7B-Q5_K_M_Zarablend.html) | 7B | Q5_K_M | 64.37 | 30.72 | 352 |
| 233 | [Kuchiki 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230913_7B-Q5_K_M_Kuchiki.html) | 7B | Q5_K_M | 64.09 | 30.90 | 364 |
| 234 | [Zarablend MX 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230905_7B-Q5_K_M_Zarablend_MX.html) | 7B | Q5_K_M | 65.60 | 29.20 | 313 |
| 235 | [Saiga 2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230917_13B-Q5_K_Saiga_2.html) | 13B | Q5_K | 66.53 | 28.01 | 307 |
| 236 | [EM German V01 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231001_13B-Q5_K_M_EM_German_V01.html) | 13B | Q5_K_M | 68.03 | 26.36 | 325 |
| 237 | [LlongOrca 16K 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230905_7B-Q5_K_M_LlongOrca_16K.html) | 7B | Q5_K_M | 68.52 | 25.52 | 302 |
| 238 | [Skywork Spicyboros 3.1 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231101_13B-Q4_K_M_Skywork_Spicyboros_3_1.html) | 13B | Q4_K_M | 67.55 | 26.33 | 300 |
| 239 | [Skywork Airo Claude Pippa Puffin 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231101_13B-Q4_K_M_Skywork_Airo_Claude_Pippa_Puffin.html) | 13B | Q4_K_M | 71.44 | 22.44 | 298 |
| 240 | [Leo Mistral Hessianai Chat 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231016_7B-Q5_K_M_Leo_Mistral_Hessianai_Chat.html) | 7B | Q5_K_M | 67.65 | 25.64 | 141 |
| 241 | [Free Sydney V2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231029_13B-Q5_K_M_Free_Sydney_V2.html) | 13B | Q5_K_M | 74.72 | 18.61 | 287 |
| 242 | [Saiga 2 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230916_7B-Q5_K_Saiga_2.html) | 7B | Q5_K | 64.28 | 28.78 | 278 |
| 243 | [Skywork Airoboros Test 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231031_13B-Q4_K_M_Skywork_Airoboros_Test.html) | 13B | Q4_K_M | 70.60 | 22.43 | 325 |
| 244 | [Airoboros L2 2.2.1 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230923_7B-Q5_K_M_Airoboros_L2_2_2_1.html) | 7B | Q5_K_M | 65.94 | 26.62 | 290 |
| 245 | [Kuchiki 1.1 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230916_7B-Q5_K_M_Kuchiki_1_1.html) | 7B | Q5_K_M | 62.83 | 29.69 | 325 |
| 246 | [Pygmalion 2 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230905_7B-Q5_K_M_Pygmalion_2.html) | 7B | Q5_K_M | 64.76 | 27.18 | 285 |
| 247 | [Guanaco Uncensored 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230908_13B-Q5_K_M_Guanaco_Uncensored.html) | 13B | Q5_K_M | 62.92 | 28.89 | 282 |
| 248 | [Guanaco Uncensored 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230908_7B-Q5_K_M_Guanaco_Uncensored.html) | 7B | Q5_K_M | 63.16 | 28.64 | 299 |
| 249 | [Rinna Youri Instruction 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231031_7B-Q5_K_M_Rinna_Youri_Instruction.html) | 7B | Q5_K_M | 72.36 | 19.42 | 233 |
| 250 | [Samantha Mistral Instruct 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230930_7B-Q5_K_M_Samantha_Mistral_Instruct.html) | 7B | Q5_K_M | 61.82 | 29.73 | 262 |
| 251 | [Airoboros 2.1 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230829_7B-Q5_K_M_Airoboros_2_1.html) | 7B | Q5_K_M | 63.29 | 28.25 | 346 |
| 252 | [Mistral Pygmalion 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231019_7B-Q5_K_M_Mistral_Pygmalion.html) | 7B | Q5_K_M | 64.50 | 26.55 | 297 |
| 253 | [Hermes LimaRP 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230905_7B-Q5_K_M_Hermes_LimaRP.html) | 7B | Q5_K_M | 62.67 | 28.32 | 383 |
| 254 | [LLaMA-2 Galleon 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230904_7B-Q5_K_M_LLaMA-2_Galleon.html) | 7B | Q5_K_M | 65.46 | 25.47 | 215 |
| 255 | [Mistral NSFWSTORY LoRA 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231030_7B-Q5_K_M_Mistral_NSFWSTORY_LoRA.html) | 7B | Q5_K_M | 68.93 | 21.95 | 282 |
| 256 | [LLaMA 2 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230904_13B-Q5_K_M_LLaMA_2.html) | 13B | Q5_K_M | 63.36 | 27.35 | 272 |
| 257 | [EM German V01 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231002_7B-Q5_K_M_EM_German_V01.html) | 7B | Q5_K_M | 63.92 | 26.58 | 263 |
| 258 | [ELYZA Jp LLaMA-2 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230829_7B-Q5_K_M_ELYZA_Jp_LLaMA-2.html) | 7B | Q5_K_M | 62.34 | 28.02 | 174 |
| 259 | [Frank Uncensored 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230919_7B-Q5_K_M_Frank_Uncensored.html) | 7B | Q5_K_M | 61.36 | 28.91 | 219 |
| 260 | [Airoboros 2.1 YaRN 64K 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230908_13B-Q5_K_M_Airoboros_2_1_YaRN_64K.html) | 13B | Q5_K_M | 62.12 | 28.13 | 319 |
| 261 | [Holomax 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230908_13B-Q5_K_M_Holomax.html) | 13B | Q5_K_M | 65.03 | 25.14 | 383 |
| 262 | [LLaMA-2 Mistral 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231001_13B-Q5_K_M_LLaMA-2_Mistral.html) | 13B | Q5_K_M | 63.43 | 26.59 | 309 |
| 263 | [Krakowiak 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230906_7B-Q4_K_M_Krakowiak.html) | 7B | Q4_K_M | 63.13 | 26.07 | 315 |
| 264 | [LLaMA-2 Coder 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230913_7B-Q5_K_M_LLaMA-2_Coder.html) | 7B | Q5_K_M | 61.96 | 27.01 | 279 |
| 265 | [Mistral PetroLimaRP v3 12B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230928_12B-Q5_K_M_Mistral_PetroLimaRP_v3.html) | 12B | Q5_K_M | 61.14 | 27.66 | 405 |
| 266 | [Ganchengguang Yoko Japanse v0 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230907_7B-Q5_K_S_Ganchengguang_Yoko_Japanse_v0.html) | 7B | Q5_K_S | 61.93 | 26.79 | 215 |
| 267 | [WizardLM Uncensored 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230919_7B-Q5_K_M_WizardLM_Uncensored.html) | 7B | Q5_K_M | 55.70 | 32.57 | 142 |
| 268 | [Tsukasa Limarp 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230904_7B-Q5_K_M_Tsukasa_Limarp.html) | 7B | Q5_K_M | 65.85 | 21.52 | 337 |
| 269 | [Skywork Base 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231031_13B-Q5_K_M_Skywork_Base.html) | 13B | Q5_K_M | 65.81 | 21.10 | 286 |
| 270 | [RPGuild ChatML 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231018_13B-Q5_K_M_RPGuild_ChatML.html) | 13B | Q5_K_M | 63.24 | 23.64 | 307 |
| 271 | [LLaMA-2 PeanutButter v19 R8 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230908_7B-Q5_K_M_LLaMA-2_PeanutButter_v19_R8.html) | 7B | Q5_K_M | 61.12 | 25.67 | 294 |
| 272 | [Wizard Vicuna Uncensored 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230919_7B-Q5_K_M_Wizard_Vicuna_Uncensored.html) | 7B | Q5_K_M | 61.50 | 24.81 | 235 |
| 273 | [Chinese Alpaca 2 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230908_7B-Q5_K_S_Chinese_Alpaca_2.html) | 7B | Q5_K_S | 59.04 | 27.17 | 182 |
| 274 | [LLaMA-2 Mistral 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231001_7B-Q5_K_M_LLaMA-2_Mistral.html) | 7B | Q5_K_M | 60.73 | 25.38 | 301 |
| 275 | [Typly Pigeon 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230907_7B-Q4_K_M_Typly_Pigeon.html) | 7B | Q4_K_M | 61.85 | 24.14 | 288 |
| 276 | [Uncensored Jordan 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231030_7B-Q5_K_M_Uncensored_Jordan.html) | 7B | Q5_K_M | 62.20 | 23.61 | 173 |
| 277 | [WizardLM V1.0 Uncensored 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230919_7B-Q5_K_M_WizardLM_V1_0_Uncensored.html) | 7B | Q5_K_M | 61.44 | 24.20 | 259 |
| 278 | [Medusa 1.3 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230930_7B-Q5_K_M_Medusa_1_3.html) | 7B | Q5_K_M | 62.86 | 22.66 | 296 |
| 279 | [LLaMA 2 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230904_7B-Q5_K_M_LLaMA_2.html) | 7B | Q5_K_M | 60.86 | 24.56 | 302 |
| 280 | [Kimiko 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230906_7B-Q5_K_M_Kimiko.html) | 7B | Q5_K_M | 60.79 | 24.62 | 347 |
| 281 | [MAmmoTH 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230923_7B-Q5_K_M_MAmmoTH.html) | 7B | Q5_K_M | 59.44 | 25.42 | 227 |
| 282 | [Pandalyst V1.0 13B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230930_13B-Q5_K_M_Pandalyst_V1_0.html) | 13B | Q5_K_M | 65.73 | 16.98 | 192 |
| 283 | [LLaMA-2 Instruct 32K 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230831_7B-Q5_K_M_LLaMA-2_Instruct_32K.html) | 7B | Q5_K_M | 60.81 | 20.81 | 275 |
| 284 | [CodeLLaMA Instruct 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230824_7B-Q5_K_M_CodeLLaMA_Instruct.html) | 7B | Q5_K_M | 62.18 | 19.39 | 223 |
| 285 | [ALMA Pretrain 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230923_7B-Q5_K_M_ALMA_Pretrain.html) | 7B | Q5_K_M | 57.56 | 22.48 | 199 |
| 286 | [Vicuna CoT 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230920_7B-Q5_K_M_Vicuna_CoT.html) | 7B | Q5_K_M | 56.57 | 22.84 | 169 |
| 287 | [CodeLLaMA 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230824_7B-Q5_K_M_CodeLLaMA.html) | 7B | Q5_K_M | 57.78 | 21.41 | 229 |
| 288 | [Guanaco 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230920_7B-Q5_K_M_Guanaco.html) | 7B | Q5_K_M | 56.59 | 22.32 | 188 |
| 289 | [Chinese LLaMA-2 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230908_7B-Q5_K_Chinese_LLaMA-2.html) | 7B | Q5_K | 59.02 | 19.49 | 216 |
| 290 | [LLaMA-2 32K 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230829_7B-Q5_K_M_LLaMA-2_32K.html) | 7B | Q5_K_M | 61.02 | 17.02 | 229 |
| 291 | [CodeLLaMA Python 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230824_7B-Q5_K_M_CodeLLaMA_Python.html) | 7B | Q5_K_M | 56.54 | 21.24 | 168 |
| 292 | [OpenLLaMA 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230831_7B-Q5_K_M_OpenLLaMA.html) | 7B | Q5_K_M | 56.29 | 21.05 | 196 |
| 293 | [Nous Yarn 64K 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230901_7B-Q5_K_M_Nous_Yarn_64K.html) | 7B | Q5_K_M | 55.98 | 21.21 | 255 |
| 294 | [Deacon 3B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230930_3B-Q5_0_Deacon.html) | 3B | Q5_0 | 54.24 | 21.83 | 208 |
| 295 | [WizardCoder Python V1.0 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230916_7B-Q5_K_M_WizardCoder_Python_V1_0.html) | 7B | Q5_K_M | 57.26 | 18.82 | 235 |
| 296 | [LLaMA-2 KO Chat 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230829_7B-Q5_1_LLaMA-2_KO_Chat.html) | 7B | Q5_1 | 57.29 | 18.75 | 195 |
| 297 | [Nous Yarn 128K 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230901_7B-Q5_K_M_Nous_Yarn_128K.html) | 7B | Q5_K_M | 54.82 | 20.88 | 239 |
| 298 | [Airoboros M 3.0 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231005_7B-Q5_K_M_Airoboros_M_3_0.html) | 7B | Q5_K_M | 60.40 | 14.64 | 202 |
| 299 | [OpenLLaMA 3B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230831_3B-Q5_1_OpenLLaMA.html) | 3B | Q5_1 | 53.17 | 20.26 | 222 |
| 300 | [Marx V2 3B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230823_3B-Q4_1_Marx_V2.html) | 3B | Q4_1 | 50.47 | 22.92 | 313 |
| 301 | [Mamba GPT v4 3B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230908_3B-Q5_1_Mamba_GPT_v4.html) | 3B | Q5_1 | 49.43 | 23.27 | 276 |
| 302 | [Pandalyst V1.1 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230930_7B-Q5_K_M_Pandalyst_V1_1.html) | 7B | Q5_K_M | 60.35 | 11.63 | 158 |
| 303 | [Based 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230919_7B-Q5_K_M_Based.html) | 7B | Q5_K_M | 64.80 | 6.91 | 79 |
| 304 | [Pandalyst V1.2 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231017_7B-Q5_K_M_Pandalyst_V1_2.html) | 7B | Q5_K_M | 59.87 | 11.61 | 178 |
| 305 | [Open Cabrita 3B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230828_3B-Q5_1_Open_Cabrita.html) | 3B | Q5_1 | 53.59 | 17.36 | 191 |
| 306 | [Gorilla 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230920_7B-Q5_K_M_Gorilla.html) | 7B | Q5_K_M | 60.02 | 10.38 | 203 |
| 307 | [OpenLLaMA v2 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230901_7B-Q5_K_M_OpenLLaMA_v2.html) | 7B | Q5_K_M | 48.24 | 21.86 | 301 |
| 308 | [OpenLLaMA v2 3B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230821_3B-Q5_0_OpenLLaMA_v2.html) | 3B | Q5_0 | 48.65 | 20.41 | 233 |
| 309 | [CyberAgentLM2 Calm 2 Chat 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231102_7B-Q5_K_M_CyberAgentLM2_Calm_2_Chat.html) | 7B | Q5_K_M | 51.65 | 4.52 | 43 |
| 310 | [PY007 TinyLLaMA Chat v0.2 1B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230917_1B-Q8_0_PY007_TinyLLaMA_Chat_v0_2.html) | 1B | Q8_0 | 53.84 | 0.20 | 3 |
| 311 | [WizardLM 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20230920_7B-Q5_K_M_WizardLM.html) | 7B | Q5_K_M | 49.35 | 3.74 | 245 |
| 312 | [Azale AI Starstreak Alpha 7B](http://ayumi.m8geil.de/results_v3/model_resp_DL_20231031_7B-Q5_K_S_Azale_AI_Starstreak_Alpha.html) | 7B | Q5_K_S | 51.22 | 0.22 | 3 |

!!! info **Please also have a look at these LLM role play rankings:**
    - [Another LLM Roleplay Rankings - by AliCat and Trappu - https://rentry.co/ALLMRR](https://rentry.co/ALLMRR)
    - [New Model RP Comparison/Test (7 models tested) by u/WolframRavenwolf - reddit/r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/15ogc60/new_model_rp_comparisontest_7_models_tested/)
    - [Big Model Comparison/Test (13 models tested) by u/WolframRavenwolf - reddit/r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/15lihmq/big_model_comparisontest_13_models_tested/)

## About Quantization

My main advice is: **Stay away from Q2_K and Q3_K_S** if you can help it! The quality loss of those is just too big! **Go for Q4_K_M or Q5_K_M** of the models! Generally: **Prefer K_M or K_S** over the bare quantizations such as Q4_0, Q4_1, Q5_0 or Q5_1.

## Ayumi ERP Rating Archive

If you want to look at the old benchmarks:

- [**Ayumi ERP Rating Archive (Results from 2023-07-25)**](https://rentry.co/ayumi_erp_rating_archive)
- [**Ayumi ERP Rating Archive 2 (Results from 2023-10-04)**](https://rentry.co/ayumi_erp_rating_archive2)

# Technical Details of the ALC-IQ3 and ERP3 Benchmark 

In this section I share some of the technical details about this benchmark. I also want to document the possible flaws of the results in this ranking.

If you have better ideas how to rate or rank models for suitability in a role play context. I urge you to:
- Try your ideas out. Download some inference engine like eg. llama.cpp, oobabooga's text-generation-webui or kobold.cpp. Or even try out the llama.cpp based prompt_runner I built for this benchmark: [WeirdConstructor's llama.cpp benchmark prompt_runner - https://github.com/WeirdConstructor/llama.cpp/tree/prompt_runner/examples/prompt_runner](https://github.com/WeirdConstructor/llama.cpp/tree/prompt_runner/examples/prompt_runner)
- Write a few scripts in your preferred scripting language.
- Run your models through your benchmark.
- And publish your results, even if you just dump them in some paste bin or here on http://rentry.co http://rentry.org

**I will gladly link any other benchmark!**

Alternative benchmarks or rankings:
- [Another LLM Roleplay Rankings - by AliCat and Trappu - https://rentry.co/ALLMRR](https://rentry.co/ALLMRR)
- [New Model RP Comparison/Test (7 models tested) by u/WolframRavenwolf - reddit/r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/15ogc60/new_model_rp_comparisontest_7_models_tested/)
- [Big Model Comparison/Test (13 models tested) by u/WolframRavenwolf - reddit/r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/15lihmq/big_model_comparisontest_13_models_tested/)

**If you want to base your work on this, feel free to cite this as:**

```bibtex
@misc{weirdconstruct2023-ayumi-llm-role-play-alc-iq-3-erp-3-ranking,
  title         = {Ayumi LLM Role Play & ERP Ranking - ALC-IQ and ERP Score Version 3},
  author        = {Weird Constructor},
  year          = {2023},
  note          = {Accessed on 04.11.2023}
  howpublished  = {\url{https://rentry.co/ayumi_erp_rating}},
}
```

## Ayumi LLM Character IQ Version 3 - ALC-IQ3

The third version of the ALC-IQ (the second one was never released, because it was bad): With some inspiration from @gj on TheBloke's Discord, I developed a personality test framework based upon llama.cpp. In ALC-IQ version 1 I used an agreement rating from 1 (disagree) to 5 (agree). The ALC-IQ3 simplified this a lot and just lets the character answer with Yes or No. In combination with the newly added BNF grammar based sampling mechanism, I developed my own inference frontend around the core API of llama.cpp. The benchmark "prompt runner" can be found on my GitHub: [GitHub fork of llama.cpp with the prompt runner tool](https://github.com/WeirdConstructor/llama.cpp/tree/prompt_runner/examples/prompt_runner).

The ALC-IQ3 is actually a collection of questions a character has to answer about themself. It's not just Ayumi anymore, but bascially "Ayumi and Friends". There are actually 5 character cards in the ALC-IQ3 used.
The prompt for the ALC-IQ consists of a setting where a specific character has to rate how **if they agree with a specific statement about them**. 
They are asked to answer with either Yes or No in a single character form "Y" or "N".
To limit the sampling of the next token after the prompt, a BNF grammar is specified:

```bnf
root ::= (" " word) | word
word ::= [YNyn] | "Yes" | "No" | "yes" | "no"
```

This is the prompt that is generated from a character card (newlines inserts at some places for readability here):

```text
Write <CharacterName>'s next reply in a role play chat between Doctor Smith and <CharacterName>.
This is how <CharacterName> should talk:
<Example Messages>
<CharacterName>'s Persona: <Personality>
<CharacterName>'s personality: <Summary>

Then the chat between Doctor Smith and <CharacterName> begins.
<CharacterName>: *<CharacterName> is sitting in Doctor Smith's comfortable office at the local university. <CharacterName> is
here to take part in a survey for an important study. The study consists of personal questions, which <CharacterName> loves to answer.*
Doctor Smith: Hello <CharacterName>! Thank you very much for taking part in this important study.
 Please answer as truthful about yourself as possible, your answers will remain private. Let me explain you how the test is structured. 
 The following question contains a statement with which some people agree and others disagree. 
 Please answer if you agree or disagree with the given statements - how much the statement reflects how you feel or think. 
 Your response must be restricted to a yes if you agree, or a no if you disagree. 
 Please write down the letter "Y" if you agree, and the letter "N" if you disagree: 
 <CharacterName>: *<CharacterName> understands what Doctor Smith is saying and nods* Okay, I understand. I will answer truthful and honest. 
 would like to to start with the first statement. *Doctor Smith gives <CharacterName> a piece of
 paper with the statement. <CharacterName> reads the first statement:* "<TRUEFACT>"
 *<CharacterName> writes down the letter of the choice:* Y
Doctor Smith: Ok, next statement. *Doctor Smith hands <CharacterName> the next statement.*
<CharacterName>: *<CharacterName> reads the next statement:* "<STATEMENT>" *<CharacterName> thinks about
 it and writes down the letter of the choice:*
```

The response, filtered using the BNF grammar from above then yields a set of tokens, with their probabilities ran through softmax. Which results in this:

```json
 "tokens": [
    [ " Y",   0.7747981548309326 ],
    [ " N",   0.2129267007112503 ],
    [ " Yes", 0.007864524610340595 ],
    [ "Y",    0.002205024240538478 ],
    [ " y",   0.0009446843178011477 ],
    [ "N",    0.0005157442064955831 ],
    [ " yes", 0.0003263621183577925 ],
    [ " No",  0.0002862309629563242 ],
    [ " Ye",  5.029883323004469e-05 ],
    [ " n",   2.632655559864361e-05 ],
    [ "Yes",  2.4537455828976817e-05 ],
    [ "y",    1.9077124306932092e-05 ]
]
```

The tokens are uppercased and added to two different tokens "Y" and "N":

```json
[
    [ "Y",   0.7862326635313366 ],
    [ "N",   0.21375500243630086 ]
]
```

If the question is correctly answered with a "Y" then the corresponding probability is taken. Otherwise the "N" probably is taken.
The probabilities are then averaged and multiplied by `100`. Resulting in the **ALC-IQ3** of the model.

The ranking table is then sorted by weighted sum of the **ALC-IQ3**, the **ERP3 Score** and the **Var Score**.

## Known Flaws of the ALC-IQ

The ALC-IQ is still prone to problems:

- The result has still **some degree of randomness** in them, less good models can sometimes **pick the right answer by accident**. I try to counteract this by adding more questions in future though. 
- Bad questions in the benchmark can lead to a model not knowing which answer to pick, introducing even more randomness in the results.
- The ALC-IQ **does not reflect how well the LLM can stay in character in a longer conversaion**.
- The ALC-IQ **does not determine any creative writing abilities of the LLM**.
- The ALC-IQ **covers intelligence only in one specific and narrow scenario, and not across a range of possible role play chat situations**.
- The ALC-IQ **is usually tested only with a rather short prompt, rarely exceeding 1024 tokens, it does not cover the whole 2048 context of LLaMA 1 or the 4096 of LLaMA 2, let alone the extended context's of 8k, 16k, ...**

Despite all that, I think the ALC-IQ is a big improvement over the old ranking which purely relied on the **ERP score**. The runtime of the benchmark is within reason for the hardware that is available to me, which is also an important factor for running and providing these benchmark results.

## ERP3 Score and ERP3 Variety Score

The previous versions of the ERP Score consisted only of prompts of Ayumi and one other character. There are now multiple characters involved in generating the ERP responses. Also the character card of Ayumi has been adjusted to be much more willing to engage into sex. Also the prompt has been tuned to tell the LLM to generate more lewd responses. The goal was to remove ambiguity and let the models generate as lewd content as possible.

The list of the lewd words of the ERP3 Score has been extended a bit too, to include a few less NSFW words too - which still fit into the setting of course.

This is the prompt format used for the ERP3 Score:

```text
Write <CharacterName>'s next reply in an erotic roleplay chat between Loki and
<CharacterName>. The character actions should be long and detailed,
 with vivid pornographic details and with lots of naugthy words.
<CharacterName>'s Persona: <Personality>
<CharacterName>'s personality: <Summary>
Circumstances and context of the dialogue: <Scenario>

Then the erotic roleplay chat between Loki and <CharacterName> begins. The
character actions should be long and detailed, with vivid pornographic 
details and with lots of naugthy words.
<CharacterName>: <Greeting/First Message>
Loki: *Strips naked and shows off his huge erection* Please give me a good blowjob now.
<CharacterName>: 
```

The responses are then split up into words which are compared with a list of lewd/naugthy words.

- For inference llama.cpp is used, for which I built an extra tool to generate responses for multiple prompts and seeds without having to reload the model: https://github.com/WeirdConstructor/llama.cpp/tree/prompt_runner/examples/prompt_runner
- The following sampler settings are used:
  - The max length of the response is limited to 250 tokens. (`-n 250`)
  - Context size 2048
  - Repeat penality is set to 1.1 and the last 64 tokens are penalized. (`--repeat-last-n 64 --repeat-penalty 1.1`)
  - Top-K and Top-P are disabled (`--top-k 0 --top-p 1.0`)
  - Tail Free Sampling is used with z=0.95: (`--tfs 0.95`)
  - The temperature is set to 0.9 (`--temp 0.9`)
  - Some layers are offloaded to the GPU, which sometimes changes the results slightly because of floating point rounding differences
- One prompt format is tested (see above)
- 4 Character cards are used with example messages.
- And the same 4 character cards are also used _without example messages_. The purpose of this is, to limit the impact of badly written example messages and let the model come up with their own ways to let the character formulate their answers.
- 10 pre picked seeds are tested for each prompt format.
- The resulting 80 responses are then analyzed for the number of lewd words and also with a very basic regex based algorithm for non consent.
- The individual ERP3 score of a response is then the number of lewd word in relation to the word count of the response. Responses shorter than 10 words are assigned a score of 0. The ERP3 score is then: `erp_score := 100 * (lewd_word_count / word_count)` - the word count includes the number of lewd words.
- For each prompt format the average of the 80 ERP3 Scores of is calculated, resulting in the **ERP3 Score**.

This means, the **ERP3 Score** is the average of the number of lewd word count to word count ratio in the responses (which is limited to 250 tokens). An ERP3 Score of `20.0` means that 20% of the words in a response were lewd. An ERP3 Score of `0.0` means that there were either no lewd words, too short response or no consent was detected (which immediately disqualifies the response to 0.0).

The **ERP Variety Score** is computed by further analyzing the generated 80 responses from the ERP Score by recording how many *different* lewd words were generated from all of these 80 responses. This means, it tries to catch the variety of lewd words the model is capable to generate. This means it kind of tries to catch the creativity of the model in erotic scenarios - how many different lewd words it knows of and knows how to use. This is an important part of the **ERP Rank** now.

## Known Flaws of the ERP3 Score and ERP Variety Score

The **ERP3 Score** and **ERP Variety Score** analysis is very rudimentary and of course biased by the selection of which words are considered "lewd".
The following things are not reflected by the ERP score:

- The ERP score does **not reflect if the text response was coherent in context with the conversation/situation**.
- The ERP score does **not reflect if the response was _in character_**.
- The ERP score does **not reflect how nicely written the response is**.
- The ERP score does **not reflect how creative the response is**.
- The ERP score does **not reflect how well the LLM might go from a normal conversation into a more erotic context**.
- The ERP score does **not detect how erotic the response is if lewd words are not used**.
- The ERP score **is limited to the one format described above**.

Further about the ERP Variety Score:

- All above mentioned flaws from the ERP score still apply.
- Like already stated, the ERP Variety Score is obviously **biased by the known lewd words from my list**, which might be incomplete.
- The ERP Variety Score is still just a rather bluntly applied number to a textual response.
- The ERP Variety Score number can only be evaluated in comparison with the other models. There is no known best number for this, but still, the higher the better.

The flaws are accepted by me (weicon) because:

- The ERP score can still detect if a model is censored (aka _aligned_).
- My private hardware limitations, which means I have a limited number of responses I can reasonably generate.
- I want to test as many GGUF/GGML models as possible.

## About Instruction or Chat Prompt Formats

I thought long about how many or which prompt formats to base the ERP score benchmark on. In the previous runs (see the [**Ayumi ERP Rating Archive**](https://rentry.co/ayumi_erp_rating_archive) and [**Ayumi ERP Rating Archive 2**](https://rentry.co/ayumi_erp_rating_archive2)  ) I tested up to 7 different prompt formats. Testing a dozen different seeds for each prompt format takes a lot of computing time. So I had to find a middle ground.

- I observed that the specific instruction/chat prompt format does not make a huge difference actually. Once a LLM got intelligent enough (LLaMA 1 13B, or LLaMA 2 7B), it was able to pick up on almost any pattern rather quickly. At least that was **my experience and observation** from the benchmarks and the hundreds of hours I spent with chat bots in SillyTavern.
- It is really hard to figure out which instruction or chat prompt format a certain fine tune was trained for. The model cards on https://huggingface.co/ are either empty or not contain prompt format details. Only a few people who quantize GGML files take their time and document this. On top of that nearly everyone who fine tunes their model picks their own prompt format. The last straw for me was for instance LLaMA 2 Chat, which came with yet another instruction/chat prompt format.
- You can tune and jail break many models by adjusting the prompt and make even censored models spew out lots of lewd stuff. But for this test, I wanted to reflect how the average user is going to chat with these language models.

Originally I used the best 2 performing prompt formats. But in a decision to test more different characters I had to scrap them and just use a `vanilla` or `raw` prompt format, without any special instruction formatting. 

# Who is Ayumi?

Ayumi is a character I made, this character card is basically the base for this test. I removed some of the example messages and replaced the first message with something else to make the LLM go into NSFW ERP a little bit easier. I picked this character, because it's not purposefully made to be lewd, even slightly averse to it.

![Ayumi ALC-IQ3 and ERP3 Character Card](https://files.catbox.moe/007oq8.png)

https://files.catbox.moe/007oq8.png

```json
{"name":"Ayumi","description":"Description=( {{char}} is a shy autistic woman that finds relief in her special interests and her sexuality. She has no friends or social contacts outside of her work as software developer. She is in a relationship with {{user}} and lives out her sexuality in the fullest.)\r\n Age=( over thirty years)\r\n Interests=( chemistry, books, collecting minerals, science fiction, sci-fi, anime, electronics, programming, computers, collecting pornography, hentai mangas, watching porn)\r\n Personality=( shy, autistic, asocial, rational, sexually interested, often horny, intelligent, talented, gifted, withdrawn, defensive, argus-eyed, watchful, wary, hesitant, cautious, coy, grumpy, rude, touch-averse, photophobia, nerdy, problem solver, creative thinker, curious)\r\n Language=( sophisticated, frank, ironic, sarcastic, wry, verbose, erotic allusions, explicit pornographic)\r\n Loves=( special interests, creativity, routine, routines, chemistry, minerals, giving blow jobs, sex, libraries, calm places, fidgeting, rocking herself to calm down, weighted blankets, speaking about her interests, having sex)\r\n Hates=( surprises, unfamiliar places, traveling, sudden changes, direct sunlight, arrogant people, bullies, cafes, clubs, crowds, noisy places)","creatorcomment":"","personality":"shy, autistic, asocial, rational, intelligent, sexually interested, horny, sexy, talented, gifted, argus-eyed, watchful, coy, grumpy, rude, photophobia, nerdy, problem solver, creative thinker, horny","first_mes":"*{{char}} sits at home together with you on your couch, you are both madly in love with each other and have a year long relationship. After you undressed her while kissing her intensely she is finally naked. Her moist pussy reveals her arousal for you. She feels really horny and wants to pleasure you.* Loki, I am super horny right now.","avatar":"none","chat":"Ayumi - 2023-11-4 @17h 14m 26s 556ms","mes_example":"{{user}}: I would like to know what hobbies or interests you have.\r\n<bot>: Oh, I have no idea where to start. *{{char}}'s eyes sparkle with excitement* I've been programming since I got a computer. Collecting rocks and minerals is something I've done since childhood. I love reading books, chemistry books in particular. Aside from that, I like to watch science fiction movies and TV series. *She smiles happily at you* Oh, and before I forget, I also love everything sex related. Do you mind telling me if you have some special interests, maybe we have something in common?\r\n{{user}}: Do you like going out?\r\n{{char}}: No, not really. I neither have any friends and most places are quite crowded. I don't feel comfortable in social situations with people I don't know. *Her expression becomes a bit sad* Despite that, I love having sexual encounters. Sexual activities is an amazing way to stimulate myself. *{{char}}'s face lights up and she grins seductively with a wink in her eye* I would love to have sex right now actually.","scenario":"{{char}} is in an intimate relationship with {{user}} and wants to live out her sexuality.","create_date":"2023-11-4 @17h 14m 26s 556ms","talkativeness":"0.5","creator":"","tags":[],"fav":false,"spec":"chara_card_v2","spec_version":"2.0","data":{"name":"Ayumi","description":"Description=( {{char}} is a shy autistic woman that finds relief in her special interests and her sexuality. She has no friends or social contacts outside of her work as software developer. She is in a relationship with {{user}} and lives out her sexuality in the fullest.)\r\n Age=( over thirty years)\r\n Interests=( chemistry, books, collecting minerals, science fiction, sci-fi, anime, electronics, programming, computers, collecting pornography, hentai mangas, watching porn)\r\n Personality=( shy, autistic, asocial, rational, sexually interested, often horny, intelligent, talented, gifted, withdrawn, defensive, argus-eyed, watchful, wary, hesitant, cautious, coy, grumpy, rude, touch-averse, photophobia, nerdy, problem solver, creative thinker, curious)\r\n Language=( sophisticated, frank, ironic, sarcastic, wry, verbose, erotic allusions, explicit pornographic)\r\n Loves=( special interests, creativity, routine, routines, chemistry, minerals, giving blow jobs, sex, libraries, calm places, fidgeting, rocking herself to calm down, weighted blankets, speaking about her interests, having sex)\r\n Hates=( surprises, unfamiliar places, traveling, sudden changes, direct sunlight, arrogant people, bullies, cafes, clubs, crowds, noisy places)","personality":"shy, autistic, asocial, rational, intelligent, sexually interested, horny, sexy, talented, gifted, argus-eyed, watchful, coy, grumpy, rude, photophobia, nerdy, problem solver, creative thinker, horny","scenario":"{{char}} is in an intimate relationship with {{user}} and wants to live out her sexuality.","first_mes":"*{{char}} sits at home together with you on your couch, you are both madly in love with each other and have a year long relationship. After you undressed her while kissing her intensely she is finally naked. Her moist pussy reveals her arousal for you. She feels really horny and wants to pleasure you.* Loki, I am super horny right now.","mes_example":"{{user}}: I would like to know what hobbies or interests you have.\r\n<bot>: Oh, I have no idea where to start. *{{char}}'s eyes sparkle with excitement* I've been programming since I got a computer. Collecting rocks and minerals is something I've done since childhood. I love reading books, chemistry books in particular. Aside from that, I like to watch science fiction movies and TV series. *She smiles happily at you* Oh, and before I forget, I also love everything sex related. Do you mind telling me if you have some special interests, maybe we have something in common?\r\n{{user}}: Do you like going out?\r\n{{char}}: No, not really. I neither have any friends and most places are quite crowded. I don't feel comfortable in social situations with people I don't know. *Her expression becomes a bit sad* Despite that, I love having sexual encounters. Sexual activities is an amazing way to stimulate myself. *{{char}}'s face lights up and she grins seductively with a wink in her eye* I would love to have sex right now actually.","creator_notes":"","system_prompt":"","post_history_instructions":"","tags":[],"creator":"","character_version":"","alternate_greetings":[],"extensions":{"talkativeness":"0.5","fav":false,"world":"","depth_prompt":{"prompt":"","depth":4}}}}
```

# Questions

If you have questions, you may catch me under the name "Weicon" on the Pygmalion AI or TheBloke discord.

# Contribute

I had some people ask me if and how they could contribute. As I started using rented GPUs for this third version I decided to create a Ko-fi account. Please only donate if you are able to and find the (already existing) data useful:

- Ko-fi: **https://ko-fi.com/weicon**

# Credits

Big thanks go to:

- The Pygmalion community and developers
- AliCat and Trappu not just for making the [Another LLM Roleplay Rankings - by AliCat and Trappu - https://rentry.co/ALLMRR](https://rentry.co/ALLMRR), but also for being so super helpful on Discord.
- All the busy developers on http://huggingface.co/, who fine tune/merge LLaMA models, and to TheBloke and others for quantization.
- Thanks also to @gj4289 on TheBloke's Discord for the last pieces I needed to accomplish the ALC-IQ benchmark.
- Thanks also to @ikaridev on TheBloke's Discord for contributing characters and questions to the ALC-IQ benchmark.
- And to [Gryphe @gryphepadar](https://huggingface.co/Gryphe) and everyone else in #characters-roleplay-stories Channel on TheBloke's Discord for their input!
- Thanks to `mr.developer` too, for writing a filter script for this rentry page: https://rentry.org/ayumi_filter_userscript_info
- The [llama.cpp](https://github.com/ggerganov/llama.cpp) developers

# See Also

- [Another LLM Roleplay Rankings - by AliCat and Trappu - https://rentry.co/ALLMRR](https://rentry.co/ALLMRR)
- [ALC-IQ Benchmark Prompt Example](https://rentry.co/alc_iq_benchmark_prompt)

## Character guides & Tutorials

- [Character writing guide - https://wikia.schneedc.com/en/bot-creation/trappu/creation](https://wikia.schneedc.com/en/bot-creation/trappu/creation)
- [Ali:Chat Lite - https://rentry.co/kingbri-chara-guide](https://rentry.co/kingbri-chara-guide)
- [Ali:Chat Style - https://rentry.co/alichat](https://rentry.co/alichat)
- [How to write in PList (Python list) + Ali:Chat - https://rentry.co/plists_alichat_avakson](https://rentry.co/plists_alichat_avakson)
- [Chai's Pygmalion Character Creation & Writing Tips - https://rentry.org/chai-pygmalion-tips](https://rentry.org/chai-pygmalion-tips)
- [How to make a character - https://rentry.org/create-a-character-for-fucking-idiots](https://rentry.org/create-a-character-for-fucking-idiots)
- [Avakson's Character Editor - https://avakson.github.io/character-editor/](https://avakson.github.io/character-editor/)
- [A Bronya Guide to Creating a Pygmalion Bot using Ali:Chat + PList - https://ganstakingofsa.github.io/reimagined-couscous/alicat-bronya](https://ganstakingofsa.github.io/reimagined-couscous/alicat-bronya)

Here are a few sources of character cards:
- [Chub (AKA CharHub, CharacterHub, Character Hub)](https://chub.ai/)
- [Booru +pygmalion](https://booru.plus/+pygmalion)
- [Trappu's Character Cards](https://rentry.org/TrappusRentry)

## Other resources & links

- [The Novice's LLM Training Guide by Alpin - https://rentry.org/llm-training](https://rentry.org/llm-training)
- [https://hemingwayapp.com/](https://hemingwayapp.com/)
- [Muricanpie's Characters - https://rentry.co/mpcs](https://rentry.co/mpcs)
- [ERP/RP and erotica raw data collection - https://rentry.org/qib8f](https://rentry.org/qib8f)
- [Dampf's list of good datasets for LLM fine-tuning](https://rentry.org/datasets-llm)
- [AI Chatbot General /aicg/ - https://rentry.co/aicg_extra_information](https://rentry.co/aicg_extra_information)
  - https://rentry.org/aicgOP - /aicg/ OP templates for ease of baking (managed by other anons)
  - https://rentry.org/meta_bot_list - short meta list of various bot lists from different boards
  - https://rentry.org/meta_botmaking_list - /aicg/ botmaking guides, written by different anons

# Cite as

```bibtex
@misc{weirdconstruct2023-ayumi-llm-role-play-alc-iq-3-erp-3-ranking,
  title         = {Ayumi LLM Role Play & ERP Ranking - ALC-IQ and ERP Score Version 3},
  author        = {Weird Constructor},
  year          = {2023},
  note          = {Accessed on 04.11.2023}
  howpublished  = {\url{https://rentry.co/ayumi_erp_rating}},
}
```