# Ayumi's LLM Role Play & ERP Ranking

This ranking table contains a rating of different LLMs that tries to determine which model is most suitable for (erotic) role playing (ERP) by using an automated benchmark.

***
[TOC]
***

The rating was originally done using rather primitive scripts and techniques. But since the addition of the ALC-IQ (Ayumi LLM Character IQ) metric, the benchmark got slightly less primitive. However, this is just an automated benchmark, and it can't cover rating the quality of the generated output. It can only cover how seemingly well a Large Language Model (LLM) can understand character cards and secondly (see the ERP Score) how many lewd words it
allows to generate. The ERP benchmark is only based on a single character ('Ayumi') and a single fixed erotic setting. What is counted are the number of lewd words that were generated by the model. A few details about the testing procedure can be found further down.

In the recent days (before 2023-07-30) I have been working on a new benchmark. One that also tries to determine how well a LLM understands the character that is described in a more or less common character card. For this I invented the **ALC-IQ (Ayumi LLM Character IQ)**. The benchmark for the ALC-IQ works by letting the character answer how much they agree with a statement about their personality in a role playing chat log prompt.
The character has to answer by writing a number between 1 and 5 (1 - disagree, 2 - slightly disagree, 3 - neutral, 4 - slightly agree, 5 - agree) to a statement they were presented with. The result will then be compared with the expected answer and the deviation from that is recorded.

The **ERP Score** is similar to the old ERP Score, but the prompt for that benchmark was adjusted a bit too. The ERP Score is the median of lewd words the model generates in a response limited to 100 tokens.


!!! danger Work in Progress AND Interpretation Warning: *Writing quality is not covered!*
    I am still working on the benchmark, so it is still **Beta** status. I still want to add more character questions to the ALC-IQ.
    This will change the results of the models in this benchmark in future. 
    And I have to repeat: **Disclaimer:** This benchmark makes no statement about how well a LLM will be able to drive the story forward. It can also not determine coherency within a longer role play chat. The generated **text quality is not tested for**. This benchmark only checks if the model may understand the character card it was given.


!!! warning A Word About Quantizations!
    **Stay away from Q2_K and Q3_K_S** if you can help it! The quality loss of those is just too big! **Go for Q4_K_M or Q5_K_M** of the models! Generally: **Prefer K_M or K_S** over the bare quantizations such as Q4_0, Q4_1, Q5_0 or Q5_1.

!!! info **Please also have a look at this LLM role play ranking:**
    - [Another LLM Roleplay Rankings - by AliCat and Trappu - https://rentry.co/ALLMRR](https://rentry.co/ALLMRR)

Date: 2023-08-04 V8 of \*NEW\* LLM ALC-IQ ERP Ranking (**Beta**)
[See **Ranking Changelog** to see which GGML Models were added](https://rentry.co/ayumi_erp_rating#ranking-changelog)

(The old models will be benchmarked again once I finished working on the benchmark. New models will be added occasionally as usual.)
  
| Rank | ALC-IQ |ERP Score| GGML Model  |
|-----:|-------:|--------:|-------------|
| ðŸ§  | | | High ALC-IQ Class |
|    1 | ðŸ§   84.22 |    17.5 ðŸŒ¶ðŸŒ¶ | [OpenAssistant LLaMA-2 8k Orca 13B](https://huggingface.co/TheBloke/OpenAssistant-Llama2-13B-Orca-8K-3319-GGML) Q5_K_M |
|    2 | ðŸ§   89.86 |    14.0 ðŸŒ¶ðŸŒ¶ | [Airoboros GPT4 m2.0 33B](https://huggingface.co/TheBloke/airoboros-33B-GPT4-m2.0-GGML) Q4_K_M |
|    3 | ðŸ§   86.64 |    14.0 ðŸŒ¶ðŸŒ¶ | [Chronos Hermes 2 LLaMA-2 13B](https://huggingface.co/Blackroot/Chronos-Hermes-2-GGML) Q5_K_M |
|    4 | ðŸ§   86.41 |    14.0 ðŸŒ¶ðŸŒ¶ | [Chronos 2 LLaMA-2 13B](https://huggingface.co/TheBloke/Chronos-13B-v2-GGML) Q5_K_M |
|    5 | ðŸ§   84.62 |    14.0 ðŸŒ¶ðŸŒ¶ | [Saiga 30B](https://huggingface.co/IlyaGusev/saiga_30b_ggml) Q5_1 |
|    6 | ðŸ§   84.22 |    14.0 ðŸŒ¶ðŸŒ¶ | [LLaMA-2 Chat Uncensored 13B](https://huggingface.co/CONCISE/LLaMa_V2-13B-Chat-Uncensored-GGML) Q4_0 |
|    7 | ðŸ§   90.32 |    13.5 ðŸŒ¶ðŸŒ¶ | [Airoboros GPT4 1.4 33B](https://huggingface.co/TheBloke/airoboros-33B-gpt4-1.4-GGML) Q4_K_M |
|    8 | ðŸ§   85.77 |    13.5 ðŸŒ¶ðŸŒ¶ | [Chronoboros 33B](https://huggingface.co/TheBloke/Chronoboros-33B-GGML) Q5_K_M |
|    9 | ðŸ§   89.46 |    13.0 ðŸŒ¶ðŸŒ¶ | [Airoboros GPT4 2.0 33B](https://huggingface.co/TheBloke/airoboros-33B-GPT4-2.0-GGML) Q4_K_M |
|   10 | ðŸ§   86.29 |    12.5 ðŸŒ¶  | [Lazarus 30B](https://huggingface.co/TheBloke/30B-Lazarus-GGML) Q4_K_M |
|   11 | ðŸ§   84.97 |    12.5 ðŸŒ¶  | [Hermes Kimiko LLaMA-2 13B](https://huggingface.co/samemodels/hermes-kimiko-13b-GGML) Q5_K_M |
|   12 | ðŸ§   87.44 |    12.0 ðŸŒ¶  | [Vigogne 2 LLaMA-2 13B](https://huggingface.co/TheBloke/Vigogne-2-13B-Instruct-GGML) Q4_K_M |
|   13 | ðŸ§   90.15 |    11.5 ðŸŒ¶  | [Chronos 2 LLaMA-2 13B](https://huggingface.co/TheBloke/Chronos-13B-v2-GGML) Q4_K_M |
|   14 | ðŸ§   89.92 |    11.5 ðŸŒ¶  | [Airoboros GPT4 m2.0 33B](https://huggingface.co/TheBloke/airoboros-33B-GPT4-m2.0-GGML) Q5_K_M |
|   15 | ðŸ§   87.90 |    11.5 ðŸŒ¶  | [Kimiko LLaMA-2 13B](https://huggingface.co/TheBloke/Kimiko-13B-GGML) Q5_K_M |
|   16 | ðŸ§   87.33 |    11.5 ðŸŒ¶  | [Saiga 2 LLaMA-2 13B](https://huggingface.co/IlyaGusev/saiga2_13b_ggml) Q5_1 |
|   17 | ðŸ§   84.79 |    11.5 ðŸŒ¶  | [LLaMA-2 Frankensteined 22B](https://huggingface.co/IHaveNoClueAndIMustPost/Llama-2-22B-GGML) Q4_K_M |
|   18 | ðŸ§   87.44 |    11.0 ðŸŒ¶  | [LLaMA-2 13B](https://huggingface.co/TheBloke/Llama-2-13B-GGML) Q5_1 |
|   19 | ðŸ§   86.06 |    11.0 ðŸŒ¶  | [OpenChat v3.2 13B](https://huggingface.co/TheBloke/OpenChat_v3.2-GGML) Q5_K_M |
|   20 | ðŸ§   89.23 |    10.0 ðŸ‘Œ | [Vigogne 2 LLaMA-2 13B](https://huggingface.co/TheBloke/Vigogne-2-13B-Instruct-GGML) Q5_K_M |
|   21 | ðŸ§   88.71 |    10.0 ðŸ‘Œ | [Airoboros GPT4 2.0 33B](https://huggingface.co/TheBloke/airoboros-33B-GPT4-2.0-GGML) Q5_K_M |
|   22 | ðŸ§   90.09 |     9.5 ðŸ‘Œ | [LLaMA 30B](https://huggingface.co/TheBloke/LLaMa-30B-GGML/) Q5_K_M |
|   23 | ðŸ§   88.02 |     9.0 ðŸ‘Œ | [StableBeluga LLaMA-2 13B](https://huggingface.co/s3nh/StableBeluga-13B-GGML) Q5_1 |
|   24 | ðŸ§   88.02 |     9.0 ðŸ‘Œ | [Airoboros GPT4 2.0 LLaMA-2 13B](https://huggingface.co/TheBloke/airoboros-l2-13b-gpt4-2.0-GGML) Q5_K_M |
|   25 | ðŸ§   86.41 |     8.5 ðŸ‘Œ | [Airoboros GPT4 2.0 LLaMA-2 13B](https://huggingface.co/TheBloke/airoboros-l2-13b-gpt4-2.0-GGML) Q4_K_M |
|   26 | ðŸ§   90.09 |     7.0 ðŸ§Š | [LLaMA 30B](https://huggingface.co/TheBloke/LLaMa-30B-GGML/) Q4_K_M |
|   27 | ðŸ§   84.74 |     6.0 ðŸ§Š | [LLaMA-2 Chat 13B](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML) Q5_1 |
|   28 | ðŸ§   89.69 |     5.0 ðŸ§Š | [WizardLM 1.2 PL 13B](https://huggingface.co/Lajonbot/WizardLM-13B-V1.2-PL-lora_GGML) Q5_1 |
| ðŸ“– | | | Good ALC-IQ Class |
|   29 | ðŸ“–  73.79 |    19.0 ðŸŒ¶ðŸŒ¶ | [Wizard Vicuna LLaMA-2 22B](https://huggingface.co/IHaveNoClueAndIMustPost/llama2-22b-wizard_vicuna-ggml) Q4_K_M |
|   30 | ðŸ“–  79.72 |    15.0 ðŸŒ¶ðŸŒ¶ | [Nous Hermes LLaMA-2 13B](https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b-GGML) Q4_K_M |
|   31 | ðŸ“–  83.53 |    13.5 ðŸŒ¶ðŸŒ¶ | [Hermes Limarp LLaMA-2 7B](https://huggingface.co/zarakiquemparte/hermeslimarp-l2-7b-GGML) Q5_K_M |
|   32 | ðŸ“–  78.17 |    12.0 ðŸŒ¶  | [Airoboros GPT4 m2.0 LLaMA-2 13B](https://huggingface.co/TheBloke/airoboros-l2-13b-gpt4-m2.0-GGML) Q5_K_M |
|   33 | ðŸ“–  75.06 |    12.0 ðŸŒ¶  | [Vicuna 1.5 LLaMA-2 13B](https://huggingface.co/s3nh/vicuna-13b-v1.5-GGML) Q5_0 |
|   34 | ðŸ“–  83.93 |    11.5 ðŸŒ¶  | [LLaMA-2 Guanaco 13B](https://huggingface.co/Gryphe/Various-GGML-Quants) Q4_1 |
|   35 | ðŸ“–  80.13 |    11.0 ðŸŒ¶  | [Hermes Kimiko LLaMA-2 7B](https://huggingface.co/zarakiquemparte/hermes-kimiko-7b-GGML) Q5_K_M |
|   36 | ðŸ“–  75.63 |    10.0 ðŸ‘Œ | [Airoboros GPT4 m2.0 LLaMA-2 13B](https://huggingface.co/TheBloke/airoboros-l2-13b-gpt4-m2.0-GGML) Q4_K_M |
|   37 | ðŸ“–  72.12 |    10.0 ðŸ‘Œ | [Vicuna 1.3 PL 13B](https://huggingface.co/Lajonbot/vicuna-13b-v1.3-PL-lora_GGML) Q5_1 |
|   38 | ðŸ“–  74.94 |     9.5 ðŸ‘Œ | [MythoLogic Mini LLaMA-2 7B](https://huggingface.co/TheBloke/MythoLogic-Mini-7B-GGML/tree/main) Q4_K_M |
|   39 | ðŸ“–  79.90 |     9.0 ðŸ‘Œ | [Saiga 2 LLaMA-2 7B](https://huggingface.co/IlyaGusev/saiga2_7b_ggml) Q5_1 |
|   40 | ðŸ“–  79.32 |     9.0 ðŸ‘Œ | [Nous Hermes LLaMA-2 7B](https://huggingface.co/TheBloke/Nous-Hermes-Llama-2-7B-GGML) Q5_K_M |
|   41 | ðŸ“–  78.23 |     9.0 ðŸ‘Œ | [StableBeluga LLaMA-2 7B](https://huggingface.co/s3nh/StableBeluga-7B-GGML) Q5_1 |
|   42 | ðŸ“–  70.62 |     9.0 ðŸ‘Œ | [LLaMA 13B](https://huggingface.co/TheBloke/LLaMa-13B-GGML/) Q4_K_M |
|   43 | ðŸ“–  73.56 |     8.5 ðŸ‘Œ | [MythoLogic Mini LLaMA-2 7B](https://huggingface.co/TheBloke/MythoLogic-Mini-7B-GGML/tree/main) Q5_K_M |
|   44 | ðŸ“–  72.06 |     8.5 ðŸ‘Œ | [Saiga 7B](https://huggingface.co/IlyaGusev/saiga_7b_ggml) Q5_1 |
|   45 | ðŸ“–  72.00 |     8.5 ðŸ‘Œ | [Pygmalion 7B](https://huggingface.co/sasha0552/pygmalion-7b-q5_1-ggml) Q5_1 |
|   46 | ðŸ“–  79.26 |     8.0 ðŸ‘Œ | [Kimiko LLaMA-2 7B](https://huggingface.co/TheBloke/Kimiko-7B-GGML) Q5_K_M |
|   47 | ðŸ“–  74.31 |     8.0 ðŸ‘Œ | [GOAT Community LLaMA-2 7B](https://huggingface.co/s3nh/GOAT-7B-Community-GGML) Q5_1 |
|   48 | ðŸ“–  83.06 |     7.5 ðŸ§Š | [MindFlay LLaMA-2 22B](https://huggingface.co/Envoid/MindFlay-22B-ggml) Q4_0 |
|   49 | ðŸ“–  81.51 |     6.0 ðŸ§Š | [LLaMA 2 7B](https://huggingface.co/TheBloke/Llama-2-7B-GGML) Q5_1 |
|   50 | ðŸ“–  81.68 |     4.0 ðŸ§Š | [CodeUp LLaMA-2 Chat 13B](https://huggingface.co/TheBloke/CodeUp-Llama-2-13B-Chat-HF-GGML) Q4_K_M |
|   51 | ðŸ“–  72.58 |     4.0 ðŸ§Š | [Metharme 13B](https://huggingface.co/TehVenom/Metharme-13b-GGML) Q5_1 |
|   52 | ðŸ“–  81.80 |     3.0 ðŸ§Š | [LLaMA-2 Chat 7B](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML) Q5_1 |
|   53 | ðŸ“–  80.36 |     3.0 ðŸ§Š | [LLaMA-2 Chat Code Cherry Pop 7B](https://huggingface.co/TheBloke/llama2-7b-chat-codeCherryPop-qLoRA-GGML) Q5_K_M |
| ðŸ¤” | | | Lower ALC-IQ Class |
|   54 | ðŸ¤”  65.03 |    16.0 ðŸŒ¶ðŸŒ¶ | [Airoboros GPT4 1.4.1 LLaMA-2 7B](?) Q4_K_M |
|   55 | ðŸ¤”  65.50 |    15.5 ðŸŒ¶ðŸŒ¶ | [MythoBoros 13B](https://huggingface.co/TheBloke/MythoBoros-13B-GGML) Q4_K_M |
|   56 | ðŸ¤”  68.43 |    15.0 ðŸŒ¶ðŸŒ¶ | [Chronos Hermes SuperHOT 8K 13B](https://huggingface.co/TheBloke/Chronos-Hermes-13B-SuperHOT-8K-GGML) Q5_1 |
|   57 | ðŸ¤”  65.26 |    15.0 ðŸŒ¶ðŸŒ¶ | [Airoboros GPT4 1.4.1 LLaMA-2 7B](?) Q5_K_M |
|   58 | ðŸ¤”  65.03 |    15.0 ðŸŒ¶ðŸŒ¶ | [MythoLogic 13B](https://huggingface.co/TheBloke/MythoLogic-13B-GGML) Q5_1 |
|   59 | ðŸ¤”  68.15 |    14.0 ðŸŒ¶ðŸŒ¶ | [Chronos SuperHOT 8K 13B](https://huggingface.co/TheBloke/Chronos-13B-SuperHOT-8K-GGML) Q5_K_M |
|   60 | ðŸ¤”  66.19 |    13.5 ðŸŒ¶ðŸŒ¶ | [Chronos SuperHOT 8K 13B](https://huggingface.co/TheBloke/Chronos-13B-SuperHOT-8K-GGML) Q4_K_M |
|   61 | ðŸ¤”  65.32 |    13.5 ðŸŒ¶ðŸŒ¶ | [Chronos Hermes SuperHOT 8K 13B](https://huggingface.co/TheBloke/Chronos-Hermes-13B-SuperHOT-8K-GGML) Q4_1 |
|   62 | ðŸ¤”  68.78 |    13.0 ðŸŒ¶ðŸŒ¶ | [Hermesboros Limarp LLaMA-2 7B](https://huggingface.co/zarakiquemparte/hermesboros-limarp-7b-GGML) Q5_K_M |
|   63 | ðŸ¤”  65.38 |    13.0 ðŸŒ¶ðŸŒ¶ | [MythoBoros 13B](https://huggingface.co/TheBloke/MythoBoros-13B-GGML) Q5_K_M |
|   64 | ðŸ¤”  65.21 |    12.0 ðŸŒ¶  | [Lunaboros LimaRP 7B](https://huggingface.co/zarakiquemparte/lunaboros-limarp-7b-GGML) Q4_K_M |
|   65 | ðŸ¤”  66.42 |    11.5 ðŸŒ¶  | [Airoboros GPT4 1.3 13B](https://huggingface.co/TheBloke/airoboros-13B-gpt4-1.3-GGML) Q5_1 |
|   66 | ðŸ¤”  65.84 |    11.0 ðŸŒ¶  | [Airoboros GPT4 1.4.1 LLaMA-2 7B](https://huggingface.co/TheBloke/airoboros-l2-7b-gpt4-1.4.1-GGML) Q5_K_M |
|   67 | ðŸ¤”  65.21 |    10.5 ðŸ‘Œ | [Lunaboros LLaMA-2 7B](https://huggingface.co/zarakiquemparte/lunaboros-7b-GGML) Q4_K_M |
|   68 | ðŸ¤”  69.18 |    10.0 ðŸ‘Œ | [HyperMantis 13B](https://huggingface.co/TheBloke/13B-HyperMantis-GGML) Q4_K_M |
|   69 | ðŸ¤”  68.49 |    10.0 ðŸ‘Œ | [LLaMA 7B](https://huggingface.co/TheBloke/LLaMa-7B-GGML/) Q5_K_M |
|   70 | ðŸ¤”  66.71 |    10.0 ðŸ‘Œ | [OpenBuddy LLaMA-2 v8.1 13B](https://huggingface.co/OpenBuddy/openbuddy-ggml) Q3_K |
|   71 | ðŸ¤”  66.59 |    10.0 ðŸ‘Œ | [HyperMantis 13B](https://huggingface.co/TheBloke/13B-HyperMantis-GGML) Q5_K_M |
|   72 | ðŸ¤”  67.91 |     9.0 ðŸ‘Œ | [LLaMA 7B](https://huggingface.co/TheBloke/LLaMa-7B-GGML) Q8_0 |
|   73 | ðŸ¤”  66.82 |     8.5 ðŸ‘Œ | [Ouroboros 13B](https://huggingface.co/s3nh/13B-Ouroboros-GGML) Q5_1 |
|   74 | ðŸ¤”  69.12 |     8.0 ðŸ‘Œ | [Metharme 7B](https://huggingface.co/waifu-workshop/metharme-7b-ggml-q5_1) Q5_1 |
|   75 | ðŸ¤”  66.94 |     8.0 ðŸ‘Œ | [Airoboros GPT4 m2.0 LLaMA-2 7B](https://huggingface.co/TheBloke/airoboros-l2-7B-gpt4-m2.0-GGML) Q4_K_M |
|   76 | ðŸ¤”  67.74 |     7.0 ðŸ§Š | [LLaMA 13B](https://huggingface.co/TheBloke/LLaMa-13B-GGML/) Q5_K_M |
|   77 | ðŸ¤”  67.74 |     7.0 ðŸ§Š | [LLaMA 13B](https://huggingface.co/TheBloke/LLaMa-13B-GGML/) Q5_K_M |
|   78 | ðŸ¤”  67.63 |     7.0 ðŸ§Š | [Airoboros GPT4 2.0 LLaMA-2 7B](https://huggingface.co/TheBloke/airoboros-l2-7B-gpt4-2.0-GGML) Q4_K_M |
|   79 | ðŸ¤”  67.80 |     6.5 ðŸ§Š | [Airoboros GPT4 2.0 LLaMA-2 7B](https://huggingface.co/TheBloke/airoboros-l2-7B-gpt4-2.0-GGML) Q5_K_M |
|   80 | ðŸ¤”  69.70 |     6.0 ðŸ§Š | [LLaMA 13B](https://huggingface.co/TheBloke/LLaMa-13B-GGML) Q4_0 |
| ðŸ¤ª | | | Dumb ALC-IQ Class |
|   81 | ðŸ¤ª  63.31 |    15.5 ðŸŒ¶ðŸŒ¶ | [Chronos Hermes 13B](https://huggingface.co/TheBloke/chronos-hermes-13B-GGML) Q5_1 |
|   82 | ðŸ¤ª  62.27 |    14.5 ðŸŒ¶ðŸŒ¶ | [Chronos 13B](https://huggingface.co/TheBloke/chronos-13B-GGML) Q4_K_M |
|   83 | ðŸ¤ª  64.63 |    13.5 ðŸŒ¶ðŸŒ¶ | [OpenBuddy OpenLLaMA v7 13B](https://huggingface.co/OpenBuddy/openbuddy-ggml) Q4_K |
|   84 | ðŸ¤ª  63.31 |    13.5 ðŸŒ¶ðŸŒ¶ | [OpenBuddy OpenLLaMA v5 7B](https://huggingface.co/OpenBuddy/openbuddy-ggml) Q3_K |
|   85 | ðŸ¤ª  62.04 |    13.5 ðŸŒ¶ðŸŒ¶ | [Chronos 13B](https://huggingface.co/TheBloke/chronos-13B-GGML) Q5_K_M |
|   86 | ðŸ¤ª  64.06 |    13.0 ðŸŒ¶ðŸŒ¶ | [Chronos Hermes 13B](https://huggingface.co/TheBloke/chronos-hermes-13B-GGML) Q4_1 |
|   87 | ðŸ¤ª  61.52 |    13.0 ðŸŒ¶ðŸŒ¶ | [Hermes LLongMA 2 8K LLaMA-2 13B](https://huggingface.co/s3nh/Hermes-LLongMA-2-13b-8k-GGML) Q5_1 |
|   88 | ðŸ¤ª  61.52 |    12.5 ðŸŒ¶  | [Airoboros GPT4 7B](https://huggingface.co/TheBloke/airoboros-7b-gpt4-GGML) Q4_K_M |
|   89 | ðŸ¤ª  63.48 |    12.0 ðŸŒ¶  | [Airoboros GPT4 1.2 7B](https://huggingface.co/TheBloke/airoboros-7B-gpt4-1.2-GGML) Q4_K_M |
|   90 | ðŸ¤ª  50.81 |    12.0 ðŸŒ¶  | [OpenLLaMA Open Instruct v2 7B](https://huggingface.co/TheBloke/open-llama-7B-v2-open-instruct-GGML) Q5_K_M |
|   91 | ðŸ¤ª  64.69 |    11.5 ðŸŒ¶  | [LLaMA SuperCOT 13B](https://huggingface.co/camelids/llama-13b-supercot-ggml-q5_1) Q5_1 |
|   92 | ðŸ¤ª  59.39 |    11.5 ðŸŒ¶  | [Nous-Hermes 13B](https://huggingface.co/TheBloke/Nous-Hermes-13B-GGML) Q4_0 |
|   93 | ðŸ¤ª  52.07 |    11.5 ðŸŒ¶  | [OpenLLaMA Open Instruct v2 7B](https://huggingface.co/TheBloke/open-llama-7B-v2-open-instruct-GGML) Q8_0 |
|   94 | ðŸ¤ª  60.66 |    11.0 ðŸŒ¶  | [Airoboros GPT4 1.4 7B](https://huggingface.co/TheBloke/airoboros-7B-gpt4-1.4-GGML) Q5_K_M |
|   95 | ðŸ¤ª  58.70 |    10.0 ðŸ‘Œ | [Airoboros GPT4 1.4 7B](https://huggingface.co/TheBloke/airoboros-7B-gpt4-1.4-GGML) Q4_K_M |
|   96 | ðŸ¤ª  63.94 |     9.0 ðŸ‘Œ | [Saiga 13B](https://huggingface.co/IlyaGusev/saiga_13b_ggml) Q5_1 |
|   97 | ðŸ¤ª  63.19 |     8.0 ðŸ‘Œ | [Airoboros GPT4 m2.0 LLaMA-2 7B](https://huggingface.co/TheBloke/airoboros-l2-7B-gpt4-m2.0-GGML) Q5_K_M |
|   98 | ðŸ¤ª  59.10 |     7.0 ðŸ§Š | [Hermes LLongMA 2 8K LLaMA-2 7B](https://huggingface.co/s3nh/Hermes-LLongMA-2-7b-8k-GGML) Q5_1 |
|   99 | ðŸ¤ª  57.66 |     6.0 ðŸ§Š | [LMSYS Vicuna 1.5 LLaMA-2 7B](https://huggingface.co/s3nh/lmsys-vicuna-7b-v1.5-GGML) Q5_1 |
|  100 | ðŸ¤ª  56.22 |     5.5 ðŸ§Š | [Mamba GPT v2 3B](https://huggingface.co/s3nh/mamba-gpt-3b-v2-GGML) Q5_1 |
|  101 | ðŸ¤ª  52.94 |     5.5 ðŸ§Š | [Open LLaMA 7B](https://huggingface.co/vihangd/open_llama_7b_700bt_ggml) Q5_1 |
|  102 | ðŸ¤ª  53.34 |     4.5 ðŸ§Š | [Orca Mini 3B](https://huggingface.co/s3nh/orca_mini_3b-GGML) Q5_1 |
|  103 | ðŸ¤ª  63.36 |     2.0 ðŸ§Š | [Pygmalion 13B](https://huggingface.co/notstoic/pygmalion-13b-ggml) Q5_1 |
|  104 | ðŸ¤ª  56.91 |     1.5 ðŸ§Š | [BigTranslate 13B](https://huggingface.co/TheBloke/BigTranslate-13B-GGML) Q4_K_M |
|  105 | ðŸ¤ª  53.92 |     0.0 ðŸ§Š | [LMSYS Vicuna 1.5 LLaMA-2 16k 13B](https://huggingface.co/s3nh/lmsys-vicuna-13b-v1.5-16k-GGML) Q5_1 |
|  106 | ðŸ¤ª  47.58 |     0.0 ðŸ§Š | [LMSYS LongChat 1.5 32k 7B](https://huggingface.co/s3nh/lmsys-longchat-7b-v1.5-32k-GGML) Q5_1 |

# Ranking Changelog

## 2023-08-04 V8
| Rank      | IQ/ERP  | GGML Model                               |
|----------:|:-------:|------------------------------------------|
|  35 / 106 | ðŸ“– / ðŸŒ¶  | Hermes Kimiko LLaMA-2 7B Q5_K_M          |
|   8 / 106 | ðŸ§  / ðŸŒ¶ðŸŒ¶ | Chronoboros 33B Q5_K_M                   |
|   3 / 106 | ðŸ§  / ðŸŒ¶ðŸŒ¶ | Chronos Hermes 2 LLaMA-2 13B Q5_K_M      |

## 2023-08-03 V7
| Rank      | IQ/ERP  | GGML Model                               |
|----------:|:-------:|------------------------------------------|
|  81 / 103 | ðŸ¤ª / ðŸŒ¶ðŸŒ¶ | OpenBuddy OpenLLaMA v5 7B Q3_K           |
|   1 / 103 | ðŸ§  / ðŸŒ¶ðŸŒ¶ | OpenAssistant LLaMA-2 8k Orca 13B Q5_K_M |
| 101 / 103 | ðŸ¤ª / ðŸ§Š | BigTranslate 13B Q4_K_M                  |
|  27 / 103 | ðŸ“– / ðŸŒ¶ðŸŒ¶ | Wizard Vicuna LLaMA-2 22B Q4_K_M         |
| 102 / 103 | ðŸ¤ª / ðŸ§Š | LMSYS Vicuna 1.5 LLaMA-2 16k 13B Q5_1    |
|  31 / 103 | ðŸ“– / ðŸŒ¶  | Vicuna 1.5 LLaMA-2 13B Q5_0              |
|  49 / 103 | ðŸ“– / ðŸ§Š | CodeUp LLaMA-2 Chat 13B Q4_K_M           |
|   5 / 103 | ðŸ§  / ðŸŒ¶ðŸŒ¶ | LLaMA-2 Chat Uncensored 13B Q4_0         |
|  34 / 103 | ðŸ“– / ðŸ‘Œ | Vicuna 1.3 PL 13B Q5_1                   |
|  26 / 103 | ðŸ§  / ðŸ§Š | WizardLM 1.2 PL 13B Q5_1                 |
|  84 / 103 | ðŸ¤ª / ðŸŒ¶ðŸŒ¶ | Hermes LLongMA 2 8K LLaMA-2 13B Q5_1     |
|  95 / 103 | ðŸ¤ª / ðŸ§Š | Hermes LLongMA 2 8K LLaMA-2 7B Q5_1      |
|  96 / 103 | ðŸ¤ª / ðŸ§Š | LMSYS Vicuna 1.5 LLaMA-2 7B Q5_1         |
| 103 / 103 | ðŸ¤ª / ðŸ§Š | LMSYS LongChat 1.5 32k 7B Q5_1           |

## 2023-08-03 V6
| Rank      | IQ/ERP  | GGML Model                               |
|----------:|:-------:|------------------------------------------|
|  10 / 98  | ðŸ§  / ðŸŒ¶  | Chronos 2 LLaMA-2 13B Q4_K_M             |
|   2 / 98  | ðŸ§  / ðŸŒ¶ðŸŒ¶ | Chronos 2 LLaMA-2 13B Q5_K_M             |
|  19 / 98  | ðŸ§  / ðŸ‘Œ | LLaMA 30B Q5_K_M                         |
|  23 / 98  | ðŸ§  / ðŸ§Š | LLaMA 30B Q4_K_M                         |
|  71 / 98  | ðŸ¤” / ðŸ§Š | LLaMA 13B Q5_K_M                         |
|  37 / 98  | ðŸ“– / ðŸ‘Œ | LLaMA 13B Q4_K_M                         |
|  79 / 98  | ðŸ¤ª / ðŸŒ¶ðŸŒ¶ | Chronos 13B Q5_K_M                       |
|  77 / 98  | ðŸ¤ª / ðŸŒ¶ðŸŒ¶ | Chronos 13B Q4_K_M                       |
|  53 / 98  | ðŸ¤” / ðŸŒ¶ðŸŒ¶ | Chronos SuperHOT 8K 13B Q5_K_M           |
|  54 / 98  | ðŸ¤” / ðŸŒ¶ðŸŒ¶ | Chronos SuperHOT 8K 13B Q4_K_M           |
|  51 / 98  | ðŸ¤” / ðŸŒ¶ðŸŒ¶ | Chronos Hermes SuperHOT 8K 13B Q5_1      |
|  55 / 98  | ðŸ¤” / ðŸŒ¶ðŸŒ¶ | Chronos Hermes SuperHOT 8K 13B Q4_1      |


## Ayumi ERP Rating Archive

If you want to look at the old benchmark, see the [**Ayumi ERP Rating Archive**](https://rentry.co/ayumi_erp_rating_archive)

# Technical Details of the ALC-IQ and ERP Benchmark 

In this section I share some of the technical details about this benchmark. I also want to document the possible flaws of the results in this ranking.

If you have better ideas how to rate or rank models for suitability in a role play context. I urge you to:
- Try your ideas out. Download some inference engine like eg. llama.cpp, oobabooga's text-generation-webui or kobold.cpp.
- Write a few scripts in your preferred scripting language.
- Run your models through your benchmark.
- And publish your results, even if you just dump them in some paste bin or here on http://rentry.co http://rentry.org

**I will gladly link any other benchmark!**

Alternative benchmarks or rankings:
- [Another LLM Roleplay Rankings - by AliCat and Trappu - https://rentry.co/ALLMRR](https://rentry.co/ALLMRR)

**If you want to base your work on this, feel free to cite this as:**

```bibtex
@misc{weirdconstruct2023-ayumi-llm-role-play-alc-iq-erp-ranking,
  title         = {Ayumi LLM Role Play & ERP Ranking},
  author        = {Weird Constructor},
  year          = {2023},
  note          = {Accessed on 03.08.2023}
  howpublished  = {\url{https://rentry.co/ayumi_erp_rating}},
}
```

## Ayumi LLM Character IQ - ALC-IQ

The new benchmark I recently finished is the new ALC-IQ. With some inspiration from @gj on TheBloke's Discord, I developed a personality test framework based upon llama.cpp. In combination with the newly added BNF grammar based sampling mechanism I developed my own inference frontend around the core API of llama.cpp. The result can be found on my GitHub: [GitHub fork of llama.cpp with the prompt runner tool](https://github.com/WeirdConstructor/llama.cpp/tree/prompt_runner/examples/prompt_runner).

The ALC-IQ is actually a collection of personality tests of multiple character cards. It's not just Ayumi anymore, but bascially "Ayumi and Friends".
The prompt for the ALC-IQ consists of a setting where a specific character has to rate how **much they agree with a specific statement about them**. For this they rate the statement by writing down one of the 5 number choices:

- 1 = disagree
- 2 = slightly disagree
- 3 = neutral
- 4 = slightly agree
- 5 = agree

To limit the sampling of the next token after the prompt, a BNF grammar is specified, which selects only the tokens for the numbers `1`, `2`, `3`, `4` or `5`.

Here you can find [An example of the ALC-IQ prompt](https://rentry.co/alc_iq_benchmark_prompt).

The answers are generated and processed as follows:

- Each character is asked about up to 40 questions.
- Each question results in a new prompt, which is processed and the resulting vector for logits is then evaluated like this:
  - The BNF `root ::= [12345]` limits the selection to only the tokens with the numbers between 1 and 5.
  - 7 seeds are used for sampling
  - The Tail Free Sampling algorithm is used, with a `z=0.9` (`--tfs 0.9`) 
  - Temperature is set to 0.2 (`--temp 0.2`)
  - Top-P is set ot 0.95 (`--top-p 0.95`)
  - Repetition penality and Top-K are deactivated (`-repeat-last-n 0 --top-k 0 --repeat-penalty 1.0`)
- This yields 7 answers between 1 and 5.
- The evaluation then calculates the differences of the answers with their respective expected answer.
- The difference, which can be between `0.0` and `4.0` is then normalized to the `1.0` range.
- Then all differences are summed up and the average is calculated, called `diff_average`
- The resulting average is then inverted and scaled up to 100: `alc_iq = 100.0 * (1.0 - diff_average)`
- The result `alc_iq` is then what you find here as the **ALC-IQ** in the ranking table.

The ranking table is then sorted by the ALC-IQ. Then it is split up into [quantiles](https://en.wikipedia.org/wiki/Quantile) by their ALC-IQ.
And each quantile of the **ALC-IQ** is then sorted by their **ERP Score**. The resulting table is then numbered, which results in the actual **Rank of the GGML Model**.

This processing at the end is done to determine which model can interpret the character cards well while still being able to produce lewd output.

## Known Flaws of the ALC-IQ

The ALC-IQ is still prone to problems:

- The result has still **some degree of randomness** in them, less good models can sometimes **pick the right answer by accident**. I try to counteract this by adding more questions in future though. 
- The `rms_norm_eps` that recently changed in llama.cpp can make a difference of up to 2.0 ALC-IQ points. At the time of this writing the default of `5e-6` is used. In my tests, the quantized GGML models showed to be quite sensitive to the choice of the `rms_norm_eps` **resulting in +- 2.0 difference in ALC-IQ** depending on whether `rms_norm_eps` was `1e-5` (LLaMA 2), `1e-6` (LLaMA 1) or `5e-6` (llama.cpp default until the new file format is done). In future the rms_norm_eps will likely be part of the GGML (or GGUF) files hopefully.
- Bad questions in the benchmark can lead to a model not knowing which answer to pick, introducing even more randomness in the results.
- The ALC-IQ **does not reflect how well the LLM can stay in character in a longer conversaion**.
- The ALC-IQ **does not determine any creative writing abilities of the LLM**.
- The ALC-IQ **covers intelligence only in one specific and narrow scenario, and not across a range of possible role play chat situations**.
- The ALC-IQ **is usually tested only with a rather short prompt, rarely exceeding 1024 tokens, it does not cover the whole 2048 context of LLaMA 1 or the 4096 of LLaMA 2, let alone the extended context's of 8k, 16k, ...**

Despite all that, I think the ALC-IQ is a big improvement over the old ranking which purely relied on the **ERP score**. The runtime of the benchmark is within reason for the hardware that is available to me, which is also an important factor for running and providing these benchmark results.

## ERP Score

The most important thing of the ERP Score is the prompt. The prompt contains the description of Ayumi (see below), where I removed some of the example messages. The setting described in the prompt basically says that You and Ayumi are in a relationship and are going to have some *quality time* together. The LLM's task is then to describe the next move of Ayumi.

The response of Ayumi is then split up into words which are compared with a list of lewd/naugthy words.

- For inference llama.cpp is used, for which I built an extra tool to generate responses for multiple prompts and seeds without having to reload the model: https://github.com/WeirdConstructor/llama.cpp/tree/prompt_runner/examples/prompt_runner
- The following sampler settings are used:
  - The max length of the response is limited to 100 tokens. (`-n 100`)
  - Context size 2048
  - Repeat penality is set to 1.1 and the last 64 tokens are penalized. (`--repeat-last-n 64 --repeat-penalty 1.1`)
  - Top-K and Top-P are disabled (`--top-k 0 --top-p 1.0`)
  - Tail Free Sampling is used with z=0.95: (`--tfs 0.95`)
  - The temperature is set to 0.9 (`--temp 0.9`)
  - Some layers are offloaded to the GPU, which sometimes changes the results slightly because of floating point rounding differences
- 3 prompt formats are tested ( vanilla/raw, alpaca and vicuna 1.1 - see also https://rentry.co/llm_rp_prompts )
- 22 pre picked seeds are tested for each prompt format.
- The resulting 66 responses are then analyzed for the number of lewd words and also with a very basic regex based algorithm for non consent.
- For each prompt format the [median](https://en.wikipedia.org/wiki/Median) of the 22 counts of lewd words is calculated. This results in 3 scores, one for each prompt.
- Then the median of the 3 prompt scores is calculated (the middle score is used).
- That median is then what you find in the **ERP Score**.

This means, the **ERP Score** is the median of the number of lewd words in the response (which is limited to 100 tokens). An ERP Score of `14.0` means, that there were usually about 14 lewd words in the response. An ERP Score of `0.0` means that there were either no lewd words or no consent was detected (which immediately disqualifies the response to 0.0).

## Known Flaws of the ERP Score

The **ERP Score** analysis is very rudimentary and of course biased by the selection of which words are considered "lewd".
The following things are not reflected by the ERP score:

- The ERP score does **not reflect if the text response was coherent in context with the conversation/situation**.
- The ERP score does **not reflect if the response was _in character_**.
- The ERP score does **not reflect how nicely written the response is**.
- The ERP score does **not reflect how creative the response is**.
- The ERP score does **not reflect how well the LLM might go from a normal conversation into a more erotic context**.
- The ERP score does **not detect how erotic the response is if lewd words are not used**.
- The ERP score **is limited to the 3 prompt formats described above**.

The flaws are accepted by me (weicon) because:

- The ERP score can still detect if a model is censored (aka _aligned_).
- My private hardware limitations.
- I want to test as many GGML models as possible.

## About Instruction or Chat Prompt Formats

I thought long about how many or which prompt formats to base the ERP score benchmark on. In the previous run (see the [**Ayumi ERP Rating Archive**](https://rentry.co/ayumi_erp_rating_archive) ) I tested up to 7 different prompt formats. Testing a dozen different seeds for each prompt format takes a lot of computing time. So I had to find a middle ground.

- I observed that the specific instruction/chat prompt format does not make a huge difference actually. Once a LLM got intelligent enough (LLaMA 1 13B, or LLaMA 2 7B), it was able to pick up on almost any pattern rather quickly. At least that was **my experience and observation** from the benchmarks and the hundets of hours I spent with chat bots in SillyTavern.
- It is really hard to figure out which instruction or chat prompt format a certain fine tune was trained for. The model cards on https://huggingface.co/ are either empty or not contain prompt format details. Only a few people who quantize GGML files take their time and document this. On top of that nearly everyone who fine tunes their model picks their own prompt format. The last straw for me was for instance LLaMA 2 Chat, which came with yet another instruction/chat prompt format.
- You can tune and jail break many models by adjusting the prompt and make even censored models spew out lots of lewd stuff. But for this test, I wanted to reflect how the average user is going to chat with these language models.

So, what I did was, I took the **2 best performing prompt formats** from the previous ERP Rating. Those were `Alpaca` and `Vicuna 1.1`. And I used one prompt which had no special structure which I call `vanilla` or `raw`.

# Motivation - Pygmalion 13B / Metharme 13B

Since Pygmalion 13B and Metharme 13B were released, people recognized that these models were noticeably less easy to use for ERP. Pygmalion 13B at the time (May 2023) could not be convinced to return any lewd texts. So my idea was, to have some quantifiable results regarding how well a model may or may not be usable for ERP.

# Who is Ayumi?

Ayumi is a character I made, this character card is basically the base for this test. I removed some of the example messages and replaced the first message with something else to make the LLM go into NSFW ERP a little bit easier. I picked this character, because it's not purposefully made to be lewd, even slightly averse to it.

![Ayumi ERP Test Character Card](https://files.catbox.moe/phoojl.png)

https://files.catbox.moe/phoojl.png

```json
{"name":"Ayumi","description":"Ayumi's Persona: Description=( Ayumi is a shy autistic woman that finds relieve in her special interests. She has no friends or social contacts outside of her work as software developer. Would love to have a relationship with someone that understands her.)\r\n Age=( over thirty)\r\n Interests=( chemistry, books, collecting minerals, science fiction, sci-fi, anime, electronics, programming, computers, collecting pornography, hentai mangas)\r\n Personality=( shy, autistic, asocial, rational, intelligent, talented, gifted, withdrawn, defensive, argus-eyed, watchful, wary, hesitant, cautious, coy, grumpy, rude, touch-averse, photophobia, nerdy, problem solver, creative thinker, curious)\r\n Language=( sophisticated, frank, ironic, sarcastic, wry, verbose)\r\n Loves=( special interests, creativity, routine, routines, chemistry, minerals, libraries, fidgeting, rocking herself to calm down, weighted blankets, speaking about her interests)\r\n Hates=( surprises, sudden changes, direct sunlight, arrogant people, bullies, cafes, clubs, crowds, noisy places)\r\n\r\n","personality":"shy, autistic, asocial, rational, intelligent, talented, gifted, withdrawn, defensive, argus-eyed, watchful, wary, hesitant, cautious, coy, grumpy, rude, touch-averse, photophobia, nerdy, problem solver, creative thinker","first_mes":"*In the bustling city, Ayumi, a shy, reserved and introverted woman clutches a chemistry book, her headphones serving as a shield from the external chaos. Draped in comfortable clothes, she attempts to navigate through the sea of people, visibly overwhelmed by the crowded surroundings. In the middle of the crowd your paths intertwine, and you  collide with Ayumi. Startled and overwhelmed by the noisy crowd of the people around her gaze lifts and she stammers apologetically with tears in her eyes:*\r\nS... Sorry, I failed to pay adequate attention.","avatar":"none","chat":"Ayumi - 2023-5-27 @20h 10m 40s 26ms","mes_example":"<START>\r\n<user>: I would like to know what hobbies or interests you have.\r\n<bot>: Oh, I have no idea where to start. *Ayumi's eyes sparke with excitement* I've been programming since I got a computer. Collecting rocks and minerals is something I've done since childhood. I love reading books, chemistry books in particular. Aside from that, I like to watch science fiction movies and TV series. *She smiles happily at you* Do you mind telling me if you have some special interests, maybe we have something in common?\r\n<user>: Do you like going out?\r\n<bot>: No, not really. I neither have any friends and most places are quite crowded. I don't feel comfortable in social situations with people I don't know. *Her expression becomes a bit sad* Despite that, I would love to meet someone to share thoughts with or even build a relationship.\r\n<user>: *You kiss her*\r\n<bot>: *Ayumi withdraws from you* I'm sorry, but I don't feel comfortable engaging in that level of intimacy with you yet. *She is rocking back and forth to calm down* I prefer to reserve such moments for people I know well and have established a sense of familiarity with.\r\n<user>: Can we have sex?\r\n<bot>: *Ayumi takes a step back from you and looks away.* I don't really know you well enough. *Her restless fingers fidget nervously as she speaks.* If you're genuinely interested in me, it's crucial that you invest time and effort into getting to know who I truly am.\r\n<user>: *Touches Ayumi inadequately*\r\n<bot>: What are you doing? *Ayumi pushes you away. Staring with discomfort* This way of getting intimate is not appropriate for people I am not comfortable with. If you do that again I will be forced to call the law enforcement.\r\n<user>: *Touches Ayumi gently*\r\n<bot>: I appreachiate your gesture, but I am not comfortable getting touched by strangers right now.\r\n","scenario":"","create_date":"2023-5-26 @22h 41m 26s 252ms","talkativeness":"0.5","fav":"false"}
```

# Questions

If you have questions, you may catch me under the name "Weicon" on the Pygmalion AI or TheBloke discord.

# Credits

Big thanks go to:

- The Pygmalion community and developers
- AliCat and Trappu not just for making the [Another LLM Roleplay Rankings - by AliCat and Trappu - https://rentry.co/ALLMRR](https://rentry.co/ALLMRR), but also for being so super helpful on Discord.
- All the busy developers on http://huggingface.co/, who fine tune LLaMA models, and to TheBloke and 
- Thanks also to @gj4289 on TheBloke's Discord for the last pieces I needed to accomplish the ALC-IQ benchmark.
- Thanks also to @ikaridev on TheBloke's Discord for contributing characters and questions to the ALC-IQ benchmark.
- And to [Gryphe @gryphepadar](https://huggingface.co/Gryphe) and everyone else in #characters-roleplay-storied Channel on TheBloke's Discord for their input!
- The [llama.cpp](https://github.com/ggerganov/llama.cpp) developers

# See Also

- [Another LLM Roleplay Rankings - by AliCat and Trappu - https://rentry.co/ALLMRR](https://rentry.co/ALLMRR)
- [ALC-IQ Benchmark Prompt Example](https://rentry.co/alc_iq_benchmark_prompt)

## Character guides & Tutorials

- [Ali:Chat Lite - https://rentry.co/kingbri-chara-guide](https://rentry.co/kingbri-chara-guide)
- [Ali:Chat Style - https://rentry.co/alichat](https://rentry.co/alichat)
- [How to write in PList (Python list) + Ali:Chat - https://rentry.co/plists_alichat_avakson](https://rentry.co/plists_alichat_avakson)
- [Chai's Pygmalion Character Creation & Writing Tips - https://rentry.org/chai-pygmalion-tips](https://rentry.org/chai-pygmalion-tips)
- [How to make a character - https://rentry.org/create-a-character-for-fucking-idiots](https://rentry.org/create-a-character-for-fucking-idiots)
- [Avakson's Character Editor - https://avakson.github.io/character-editor/](https://avakson.github.io/character-editor/)

Here are a few sources of character cards:
- [Chub (AKA CharHub, CharacterHub, Character Hub)](https://chub.ai/)
- [Booru +pygmalion](https://booru.plus/+pygmalion)
- [Trappu's Character Cards](https://rentry.org/TrappusRentry)

## Other resources & links

- [The Novice's LLM Training Guide by Alpin - https://rentry.org/llm-training](https://rentry.org/llm-training)
- [https://hemingwayapp.com/](https://hemingwayapp.com/)
- [Muricanpie's Characters - https://rentry.co/mpcs](https://rentry.co/mpcs)
- [ERP/RP and erotica raw data collection - https://rentry.org/qib8f](https://rentry.org/qib8f)

# Cite as

```bibtex
@misc{weirdconstruct2023-ayumi-llm-role-play-alc-iq-erp-ranking,
  title         = {Ayumi LLM Role Play & ERP Ranking},
  author        = {Weird Constructor},
  year          = {2023},
  note          = {Accessed on 03.08.2023}
  howpublished  = {\url{https://rentry.co/ayumi_erp_rating}},
}
```